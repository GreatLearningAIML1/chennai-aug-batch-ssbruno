{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFfDTfhlaEI_"
   },
   "source": [
    "# Transfer Learning MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rNwbqCFRaEJC"
   },
   "source": [
    "* Train a simple convnet on the MNIST dataset the first 5 digits [0..4].\n",
    "* Freeze convolutional layers and fine-tune dense layers for the classification of digits [5..9]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUB1uDW_8XIy"
   },
   "source": [
    "## 1. Import necessary libraries for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rsj4t5HTaEJE"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IXrn3heBaEJa"
   },
   "source": [
    "## 2. Import MNIST data and create 2 datasets with one dataset having digits from 0 to 4 and other from 5 to 9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pjDuiK6ztgOK"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def break_data(x, y):\n",
    "    x_4, x_9, y_4, y_9 = [], [], [], []\n",
    "    for i in range(0, len(x)):\n",
    "        if(y[i] < 5):\n",
    "            x_4.append(x[i])\n",
    "            y_4.append(y[i])\n",
    "        else:\n",
    "            x_9.append(x[i])\n",
    "            y_9.append(y[i])\n",
    "    x_4, x_9, y_4, y_9\n",
    "    return (np.asarray(x_4), np.asarray(x_9), np.asarray(y_4), np.asarray(y_9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train_4, x_train_9, y_train_4, y_train_9) = break_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_test_4, x_test_9, y_test_4, y_test_9) = break_data(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qU14lYL9A5g"
   },
   "source": [
    "## 3. Print x_train, y_train, x_test and y_test for both the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z9OrszhJ0SgJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train Data 0 to 4: X: 30596\n",
      "Y Train Data 0 to 4: X: 30596\n",
      "X Test Data 0 to 4: X: 5139\n",
      "Y Test Data 0 to 4: X: 5139\n"
     ]
    }
   ],
   "source": [
    "print('X Train Data 0 to 4: X:',len(x_train_4))\n",
    "print('Y Train Data 0 to 4: X:',len(y_train_4))\n",
    "print('X Test Data 0 to 4: X:',len(x_test_4))\n",
    "print('Y Test Data 0 to 4: X:',len(y_test_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sJswV4xk9jQS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train Data 5 to 9: X: 29404\n",
      "Y Train Data 5 to 9: X: 29404\n",
      "X Test Data 5 to 9: X: 4861\n",
      "Y Test Data 5 to 9: X: 4861\n"
     ]
    }
   ],
   "source": [
    "print('X Train Data 5 to 9: X:',len(x_train_9))\n",
    "print('Y Train Data 5 to 9: X:',len(y_train_9))\n",
    "print('X Test Data 5 to 9: X:',len(x_test_9))\n",
    "print('Y Test Data 5 to 9: X:',len(y_test_9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cB9BPFzr9oDF"
   },
   "source": [
    "## ** 4. Let us take only the dataset (x_train, y_train, x_test, y_test) for Integers 0 to 4 in MNIST **\n",
    "## Reshape x_train and x_test to a 4 Dimensional array (channel = 1) to pass it into a Conv2D layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FlQRPfFzaEJx"
   },
   "outputs": [],
   "source": [
    "x_train_4 = x_train_4.reshape(x_train_4.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test_4 = x_test_4.reshape(x_test_4.shape[0], 28, 28, 1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jLQr-b3F-hw8"
   },
   "source": [
    "## 5. Normalize x_train and x_test by dividing it by 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PlEZIAG5-g2I"
   },
   "outputs": [],
   "source": [
    "x_train_4 /= 255\n",
    "x_test_4 /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pytVBaw4-vMi"
   },
   "source": [
    "## 6. Use One-hot encoding to divide y_train and y_test into required no of output classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V48xiua4-uUi"
   },
   "outputs": [],
   "source": [
    "y_train_4 = np_utils.to_categorical(y_train_4, 10)\n",
    "y_test_4 = np_utils.to_categorical(y_test_4, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "elPkI44g_C2b"
   },
   "source": [
    "## 7. Build a sequential model with 2 Convolutional layers with 32 kernels of size (3,3) followed by a Max pooling layer of size (2,2) followed by a drop out layer to be trained for classification of digits 0-4  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MU09mm9F89gO"
   },
   "outputs": [],
   "source": [
    "# Define model\n",
    "model = Sequential()\n",
    "\n",
    "# 1st Conv Layer\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(28, 28, 1)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# 2nd Conv Layer\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sJQaycRO_3Au"
   },
   "source": [
    "## 8. Post that flatten the data and add 2 Dense layers with 128 neurons and neurons = output classes with activation = 'relu' and 'softmax' respectively. Add dropout layer inbetween if necessary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOZeRbK7t9AT"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AIML\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(units=10, kernel_initializer=\"he_normal\", use_bias=True)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\AIML\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/10\n",
      "30596/30596 [==============================] - 62s 2ms/step - loss: 0.0736 - acc: 0.9792 - val_loss: 0.0161 - val_acc: 0.9953\n",
      "Epoch 2/10\n",
      "30596/30596 [==============================] - 61s 2ms/step - loss: 0.0187 - acc: 0.9941 - val_loss: 0.0250 - val_acc: 0.9930\n",
      "Epoch 3/10\n",
      "30596/30596 [==============================] - 61s 2ms/step - loss: 0.0105 - acc: 0.9963 - val_loss: 0.0130 - val_acc: 0.9953\n",
      "Epoch 4/10\n",
      "30596/30596 [==============================] - 60s 2ms/step - loss: 0.0078 - acc: 0.9974 - val_loss: 0.0144 - val_acc: 0.9955\n",
      "Epoch 5/10\n",
      "30596/30596 [==============================] - 60s 2ms/step - loss: 0.0059 - acc: 0.9985 - val_loss: 0.0069 - val_acc: 0.9982\n",
      "Epoch 6/10\n",
      "30596/30596 [==============================] - 59s 2ms/step - loss: 0.0057 - acc: 0.9982 - val_loss: 0.0112 - val_acc: 0.9963\n",
      "Epoch 7/10\n",
      "30596/30596 [==============================] - 60s 2ms/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.0045 - val_acc: 0.9984\n",
      "Epoch 8/10\n",
      "30596/30596 [==============================] - 63s 2ms/step - loss: 0.0032 - acc: 0.9989 - val_loss: 0.0043 - val_acc: 0.9982\n",
      "Epoch 9/10\n",
      "30596/30596 [==============================] - 60s 2ms/step - loss: 0.0021 - acc: 0.9995 - val_loss: 0.0042 - val_acc: 0.9982\n",
      "Epoch 10/10\n",
      "30596/30596 [==============================] - 61s 2ms/step - loss: 0.0022 - acc: 0.9994 - val_loss: 0.0078 - val_acc: 0.9979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221d3f67780>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fully Connected Layer\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "#Batch Normalisation\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "\n",
    "# Prediction Layer\n",
    "model.add(Dense(output_dim=10, init='he_normal', bias=True))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Loss and Optimizer\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "# Store Training Results\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, verbose=1, mode='auto')\n",
    "callback_list = [early_stopping]\n",
    "\n",
    "model.fit(x_train_4, y_train_4, batch_size=100, nb_epoch=10,\n",
    "           validation_data=(x_test_4, y_test_4), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "my1P09bxAv8H"
   },
   "source": [
    "## 9. Print the training and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yf7F8Gdutbf0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5139/5139 [==============================] - 4s 693us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.007768147450103117, 0.9978595057404164]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(x_test_4, y_test_4)\n",
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z78o3WIjaEJ3"
   },
   "source": [
    "## 10. Make only the dense layers to be trainable and convolutional layers to be non-trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "brN7VZHFaEJ4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.layers.convolutional.Conv2D at 0x221d4f3cac8>,\n",
       " <keras.layers.core.Activation at 0x221d3800b38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x221d3b10860>,\n",
       " <keras.layers.core.Activation at 0x221d43fdb70>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x221d3dbf828>,\n",
       " <keras.layers.core.Dropout at 0x221dc4b4908>,\n",
       " <keras.layers.core.Flatten at 0x221d3dd0908>,\n",
       " <keras.layers.core.Dense at 0x221d3dd09e8>,\n",
       " <keras.layers.core.Activation at 0x221d3dd0d30>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x221dc5dc588>,\n",
       " <keras.layers.core.Dense at 0x221dc5dcb70>,\n",
       " <keras.layers.core.Activation at 0x221dc5dc7b8>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for layer in model.layers:\n",
    "    cnt += 1\n",
    "    if cnt > 6:\n",
    "        layer.trainable = False\n",
    "    else:\n",
    "        layer.trainable = True\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4opnW7o0BJ8P"
   },
   "source": [
    "## 11. Use the model trained on 0 to 4 digit classification and train it on the dataset which has digits 5 to 9  (Using Transfer learning keeping only the dense layers to be trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_9 = x_train_9.reshape(x_train_9.shape[0], 28, 28, 1).astype('float32')\n",
    "x_test_9 = x_test_9.reshape(x_test_9.shape[0], 28, 28, 1).astype('float32')\n",
    "x_train_9 /= 255\n",
    "x_test_9 /= 255\n",
    "y_train_9 = np_utils.to_categorical(y_train_9, 10)\n",
    "y_test_9 = np_utils.to_categorical(y_test_9, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lCFcYHTm6-cE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\AIML\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 29404 samples, validate on 4861 samples\n",
      "Epoch 1/10\n",
      "29404/29404 [==============================] - 56s 2ms/step - loss: 0.0314 - acc: 0.9911 - val_loss: 0.0741 - val_acc: 0.9805\n",
      "Epoch 2/10\n",
      "29404/29404 [==============================] - 56s 2ms/step - loss: 0.0306 - acc: 0.9909 - val_loss: 0.0501 - val_acc: 0.9901\n",
      "Epoch 3/10\n",
      "29404/29404 [==============================] - 57s 2ms/step - loss: 0.0290 - acc: 0.9915 - val_loss: 0.0638 - val_acc: 0.9864\n",
      "Epoch 4/10\n",
      "29404/29404 [==============================] - 55s 2ms/step - loss: 0.0257 - acc: 0.9922 - val_loss: 0.0650 - val_acc: 0.9875\n",
      "Epoch 5/10\n",
      "29404/29404 [==============================] - 55s 2ms/step - loss: 0.0257 - acc: 0.9929 - val_loss: 0.1277 - val_acc: 0.9726\n",
      "Epoch 6/10\n",
      "29404/29404 [==============================] - 54s 2ms/step - loss: 0.0280 - acc: 0.9917 - val_loss: 0.0714 - val_acc: 0.9879\n",
      "Epoch 7/10\n",
      "29404/29404 [==============================] - 54s 2ms/step - loss: 0.0242 - acc: 0.9934 - val_loss: 0.0730 - val_acc: 0.9872\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x221d6d50da0>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train_9, y_train_9, batch_size=100, nb_epoch=10,\n",
    "           validation_data=(x_test_9, y_test_9), callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoDozqghCJZ4"
   },
   "source": [
    "## 12. Print the accuracy for classification of digits 5 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9fCxgb5s49Cj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4861/4861 [==============================] - 3s 701us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07296136675398307, 0.98724542275252]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_and_metrics = model.evaluate(x_test_9, y_test_9)\n",
    "loss_and_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FU-HwvIdH0M-"
   },
   "source": [
    "## Sentiment analysis <br> \n",
    "\n",
    "The objective of the second problem is to perform Sentiment analysis from the tweets data collected from the users targeted at various mobile devices.\n",
    "Based on the tweet posted by a user (text), we will classify if the sentiment of the user targeted at a particular mobile device is positive or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nAQDiZHRH0M_"
   },
   "source": [
    "### 13. Read the dataset (tweets.csv) and drop the NA's while reading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3eXGIe-SH0NA"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWeWe1eJH0NF"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \n",
       "0                                   Negative emotion  \n",
       "1                                   Positive emotion  \n",
       "2                                   Positive emotion  \n",
       "3                                   Negative emotion  \n",
       "4                                   Positive emotion  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('tweets.csv', encoding='ISO-8859-1')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jPJvTjefH0NI"
   },
   "source": [
    "### 14. Preprocess the text and add the preprocessed text in a column with name `text` in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5iec5s9gH0NI"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    try:\n",
    "        etext = text.encode('ascii')\n",
    "        return etext.decode('ascii')\n",
    "    except Exception as e:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EQSmqA-vH0NT"
   },
   "outputs": [],
   "source": [
    "data['text'] = [preprocess(text) for text in data.tweet_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7kX-WoJDH0NV"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>emotion_in_tweet_is_directed_at</th>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>.@wesley83 I have a 3G iPhone. After 3 hrs twe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@jessedee Know about @fludapp ? Awesome iPad/i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "      <td>iPad</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@swonderlin Can not wait for #iPad 2 also. The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "      <td>iPad or iPhone App</td>\n",
       "      <td>Negative emotion</td>\n",
       "      <td>@sxsw I hope this year's festival isn't as cra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "      <td>Google</td>\n",
       "      <td>Positive emotion</td>\n",
       "      <td>@sxtxstate great stuff on Fri #SXSW: Marissa M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text  \\\n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...   \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...   \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...   \n",
       "3  @sxsw I hope this year's festival isn't as cra...   \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...   \n",
       "\n",
       "  emotion_in_tweet_is_directed_at  \\\n",
       "0                          iPhone   \n",
       "1              iPad or iPhone App   \n",
       "2                            iPad   \n",
       "3              iPad or iPhone App   \n",
       "4                          Google   \n",
       "\n",
       "  is_there_an_emotion_directed_at_a_brand_or_product  \\\n",
       "0                                   Negative emotion   \n",
       "1                                   Positive emotion   \n",
       "2                                   Positive emotion   \n",
       "3                                   Negative emotion   \n",
       "4                                   Positive emotion   \n",
       "\n",
       "                                                text  \n",
       "0  .@wesley83 I have a 3G iPhone. After 3 hrs twe...  \n",
       "1  @jessedee Know about @fludapp ? Awesome iPad/i...  \n",
       "2  @swonderlin Can not wait for #iPad 2 also. The...  \n",
       "3  @sxsw I hope this year's festival isn't as cra...  \n",
       "4  @sxtxstate great stuff on Fri #SXSW: Marissa M...  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OGWB3P2WH0NY"
   },
   "source": [
    "### 15. Consider only rows having Positive emotion and Negative emotion and remove other rows from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bdgA_8N2H0NY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Negative emotion', 'Positive emotion',\n",
       "       'No emotion toward brand or product', \"I can't tell\"], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.is_there_an_emotion_directed_at_a_brand_or_product.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_Jlu-reIH0Na"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_there_an_emotion_directed_at_a_brand_or_product</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I can't tell</th>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Negative emotion</th>\n",
       "      <td>570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>No emotion toward brand or product</th>\n",
       "      <td>5388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Positive emotion</th>\n",
       "      <td>2978</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    tweet_text\n",
       "is_there_an_emotion_directed_at_a_brand_or_product            \n",
       "I can't tell                                               156\n",
       "Negative emotion                                           570\n",
       "No emotion toward brand or product                        5388\n",
       "Positive emotion                                          2978"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby(by='is_there_an_emotion_directed_at_a_brand_or_product').agg({'tweet_text':'count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets with either positive/negative emotions: 3548\n"
     ]
    }
   ],
   "source": [
    "data = data[data.is_there_an_emotion_directed_at_a_brand_or_product.isin(['Positive emotion','Negative emotion'])]\n",
    "print('Tweets with either positive/negative emotions:',data.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SotCRvkDH0Nf"
   },
   "source": [
    "### 16. Represent text as numerical data using `CountVectorizer` and get the document term frequency matrix\n",
    "\n",
    "#### Use `vect` as the variable name for initialising CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YcbkY4sgH0Ng"
   },
   "outputs": [],
   "source": [
    "tweet_data = np.asarray(data.text)\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KyXtZGr-H0Nl"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>0310apple</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100s</th>\n",
       "      <th>100tc</th>\n",
       "      <th>101</th>\n",
       "      <th>...</th>\n",
       "      <th>zimride</th>\n",
       "      <th>zing</th>\n",
       "      <th>zip</th>\n",
       "      <th>zite</th>\n",
       "      <th>zms</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zzzs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 5850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  02  03  0310apple  08  10  100  100s  100tc  101  ...   zimride  zing  \\\n",
       "0    0   0   0          0   0   0    0     0      0    0  ...         0     0   \n",
       "1    0   0   0          0   0   0    0     0      0    0  ...         0     0   \n",
       "2    0   0   0          0   0   0    0     0      0    0  ...         0     0   \n",
       "\n",
       "   zip  zite  zms  zombies  zomg  zone  zoom  zzzs  \n",
       "0    0     0    0        0     0     0     0     0  \n",
       "1    0     0    0        0     0     0     0     0  \n",
       "2    0     0    0        0     0     0     0     0  \n",
       "\n",
       "[3 rows x 5850 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Term Frequency\n",
    "tf = pd.DataFrame(vect.fit_transform(tweet_data).toarray(), columns=vect.get_feature_names())\n",
    "tf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4LUM-XPH0Nn"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>0310apple</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100s</th>\n",
       "      <th>100tc</th>\n",
       "      <th>101</th>\n",
       "      <th>...</th>\n",
       "      <th>zimride</th>\n",
       "      <th>zing</th>\n",
       "      <th>zip</th>\n",
       "      <th>zite</th>\n",
       "      <th>zms</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zzzs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000  02  03  0310apple  08  10  100  100s  100tc  101  ...   zimride  zing  \\\n",
       "0    7   1   2          1   1  17    5     1      1    4  ...         1     1   \n",
       "\n",
       "   zip  zite  zms  zombies  zomg  zone  zoom  zzzs  \n",
       "0    1     1    2        2     5     1     2     1  \n",
       "\n",
       "[1 rows x 5850 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document Frequency\n",
    "vect = CountVectorizer(binary=True)\n",
    "df = vect.fit_transform(tweet_data).toarray().sum(axis=0)\n",
    "pd.DataFrame(df.reshape(1, 5850), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aIdZYxJtH0Nq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>000</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>0310apple</th>\n",
       "      <th>08</th>\n",
       "      <th>10</th>\n",
       "      <th>100</th>\n",
       "      <th>100s</th>\n",
       "      <th>100tc</th>\n",
       "      <th>101</th>\n",
       "      <th>...</th>\n",
       "      <th>zimride</th>\n",
       "      <th>zing</th>\n",
       "      <th>zip</th>\n",
       "      <th>zite</th>\n",
       "      <th>zms</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zomg</th>\n",
       "      <th>zone</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zzzs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5850 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   000   02   03  0310apple   08   10  100  100s  100tc  101  ...   zimride  \\\n",
       "0  0.0  0.0  0.0        0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "1  0.0  0.0  0.0        0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "2  0.0  0.0  0.0        0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "3  0.0  0.0  0.0        0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "4  0.0  0.0  0.0        0.0  0.0  0.0  0.0   0.0    0.0  0.0  ...       0.0   \n",
       "\n",
       "   zing  zip  zite  zms  zombies  zomg  zone  zoom  zzzs  \n",
       "0   0.0  0.0   0.0  0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0  0.0   0.0  0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0  0.0   0.0  0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0  0.0   0.0  0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0  0.0   0.0  0.0      0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 5850 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term Frequency-Inverse Document Frequency \n",
    "tf_df = tf/df\n",
    "tf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5pxd5fSHH0Nt"
   },
   "source": [
    "### 17. Find number of different words in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p1DQ2LdNH0Nu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['000', '02', '03', '0310apple', '08', '10', '100', '100s', '100tc', '101', '106', '10am', '10k', '10mins', '10pm', '10x', '11', '11ntc', '11th', '12', '120', '12b', '12th', '13', '130', '14', '1406', '1413', '1415', '15', '150', '1500', '150m', '157', '15am', '15k', '15slides', '16162', '169', '16gb', '16mins', '17', '188', '1986', '1990style', '1991', '1k', '1m', '1pm', '1st', '20', '200', '2010', '2011', '2012', '20s', '21', '210', '22', '23', '24', '25', '250k', '25th', '2am', '2day', '2honor', '2moro', '2nd', '2nite', '2s', '2yrs', '30', '300', '3000', '30a', '30am', '30p', '30pm', '310409h2011', '32', '32gb', '35', '36', '360', '37', '3blks', '3d', '3g', '3gs', '3k', '3rd', '3x', '40', '400', '40min', '41', '437', '45', '45am', '47', '48', '4am', '4android', '4chan', '4g', '4nqv92l', '4sq', '4sq3', '4square', '50', '54', '55', '58', '59', '59pm', '5hrs', '5pm', '5th', '60', '64g', '64gb', '64gig', '64mb', '65', '6hours', '6th', '70', '75', '7th', '80', '800', '80s', '81', '82', '83323324', '83323414', '89', '8am', '8p', '8pm', '90', '900', '911tweets', '95', '96', '967', '97', '98', '99', '9th', '______', '_______', 'a3xvwc6', 'aapl', 'abacus', 'abandoned', 'ability', 'able', 'about', 'above', 'abroad', 'absolute', 'absolutely', 'abt', 'abuzz', 'academy', 'acc', 'acceptable', 'access', 'accessibility', 'accessible', 'accessories', 'accessory', 'accesssxsw', 'accommodate', 'according', 'accordion', 'account', 'acerbic', 'achieve', 'acknowledge', 'aclu', 'aclus', 'acquired', 'across', 'acrosse', 'acting', 'action', 'actions', 'activate', 'activations', 'activity', 'actors', 'actsofsharing', 'actual', 'actually', 'ad', 'adam', 'adapt', 'adaptive', 'add', 'added', 'addicted', 'addictedtotheinterwebs', 'addiction', 'addictive', 'addicts', 'adding', 'addition', 'additional', 'address', 'admired', 'admission', 'admit', 'admits', 'admitting', 'ado', 'adopter', 'adopters', 'adoption', 'adpeopleproblems', 'ads', 'advanced', 'advantage', 'advent', 'adventure', 'advertising', 'advice', 'advisory', 'aesthetic', 'affair', 'affirmative', 'afford', 'afraid', 'africans', 'after', 'afternoon', 'again', 'against', 'agchat', 'age', 'agencies', 'agency', 'agenda', 'agents', 'agileagency', 'agnerd', 'ago', 'agree', 'agreed', 'ah', 'ahead', 'ahem', 'ahh', 'ahhh', 'ahing', 'aicn', 'aiding', 'aim', 'ain', 'air', 'airline', 'airlines', 'airplane', 'airport', 'airports', 'airs', 'ajs2011', 'aka', 'akqas', 'al', 'alamo', 'alan', 'alarm', 'alarms', 'alas', 'album', 'alert', 'alerts', 'alex', 'algorithm', 'alive', 'all', 'allhat', 'allhat3', 'allow', 'allowing', 'allows', 'almost', 'alone', 'along', 'alot', 'alphagraphics', 'already', 'also', 'alt', 'alternate', 'alternative', 'although', 'altimeter', 'always', 'alwayshavingtoplugin', 'am', 'amateurhour', 'amazing', 'amazingly', 'amazon', 'ambassador', 'amble', 'amen', 'america', 'amex', 'amid', 'amigos', 'amismarternow', 'among', 'amount', 'amp', 'amused', 'amusing', 'an', 'analysis', 'analytics', 'and', 'andoid', 'andrew', 'andriod', 'andro', 'android', 'androidsxsw', 'angel', 'angry', 'angrybirds', 'animation', 'announce', 'announced', 'announcement', 'announcements', 'announces', 'announcing', 'annoyed', 'annoying', 'another', 'answer', 'answered', 'anti', 'anticipate', 'antigov', 'antique', 'antonio', 'antwoord', 'anxiety', 'anxious', 'any', 'anybody', 'anybodywanttobuymeanipad2', 'anymore', 'anyone', 'anyones', 'anything', 'anyway', 'anyways', 'anywhere', 'aos', 'ap', 'apac', 'apartment', 'api', 'apis', 'apologies', 'app', 'apparent', 'apparently', 'appcircus', 'appeal', 'appealing', 'appear', 'appears', 'applauds', 'applause', 'apple', 'apple_store', 'appleaddiction', 'appleatxdt', 'applefanatic', 'apples', 'appletakingoverworld', 'appletogo', 'application', 'applications', 'appolicious', 'appreciate', 'appreciation', 'approaches', 'approval', 'approved', 'approves', 'apps', 'appstore', 'aquent', 'arcade', 'archive', 'arctic', 'arduino', 'are', 'area', 'areas', 'aren', 'arg', 'argues', 'argument', 'aristotle', 'arm', 'armadillo', 'armed', 'aron', 'around', 'arrived', 'arrives', 'arriving', 'arsenal', 'art', 'article', 'articles', 'articulate', 'artificial', 'artist', 'artistic', 'artists', 'artwork', 'artworks', 'arw', 'as', 'asddieu', 'ask', 'asked', 'asking', 'asks', 'asleep', 'ass', 'assistivetech', 'assume', 'at', 'atari', 'atl', 'atms', 'atrix', 'att', 'attached', 'attempt', 'attend', 'attended', 'attendees', 'attending', 'attention', 'attitudes', 'attracted', 'attracting', 'attractive', 'atx', 'atzip', 'audience', 'audio', 'augcomm', 'augmented', 'augmentedreality', 'auntie', 'aus', 'austin', 'austincrowd', 'austinites', 'austintx', 'austinwins', 'australian', 'ausxsw', 'auth', 'authenticator', 'authorization', 'autistic', 'auto', 'autocorrect', 'autocorrected', 'autocorrects', 'autodial', 'automatically', 'autonomous', 'avail', 'available', 'ave', 'avenue', 'average', 'averages', 'avoid', 'avoiding', 'aw', 'awake', 'award', 'awards', 'aware', 'awareness', 'away', 'awe', 'awesome', 'awesomely', 'awesomeness', 'awesometiming', 'awhile', 'awkward', 'awwww', 'axzwxb', 'b4', 'baby', 'back', 'background', 'backlight', 'backpack', 'backup', 'backupify', 'bad', 'badge', 'badger', 'badges', 'bag', 'bags', 'bahahahaha', 'bajillions', 'balance', 'balckberries', 'balcony', 'ball', 'ballroom', 'ballrooms', 'banality', 'band', 'bands', 'bandwaggoners', 'bandwidth', 'bang', 'banged', 'bank', 'banking', 'bankinnovate', 'bankinnovation', 'banks', 'bar', 'barcode', 'barely', 'barging', 'barring', 'barroom', 'barry', 'barrydiller', 'bars', 'bart', 'barton', 'based', 'bashing', 'basic', 'basically', 'basics', 'basis', 'basket', 'bastards', 'bat', 'bathroom', 'batphone', 'batt', 'batteries', 'battery', 'batterykiller', 'battle', 'battledecks', 'battlela', 'bavc', 'bavcid', 'bawling', 'bb', 'bbq', 'bc', 'bday', 'be', 'beach', 'beans', 'bear', 'beard', 'beards', 'beat', 'beats', 'beautiful', 'beautifully', 'beauty', 'because', 'become', 'becoming', 'bed', 'beechwood', 'been', 'beer', 'before', 'beforetwitter', 'begin', 'beginning', 'begins', 'behance', 'behave', 'behaving', 'behavior', 'behind', 'being', 'believe', 'belinsky', 'belong', 'beluga', 'bemyneighbor', 'ben', 'benefit', 'benieuwd', 'bereft', 'bergstrom', 'berklee', 'berkowitz', 'bernd', 'berry', 'best', 'bestappever', 'bestie', 'bet', 'beta', 'betainvites', 'better', 'bettercloud', 'bettersearch', 'betterthingstodo', 'between', 'beware', 'beyond', 'beyondwc', 'bff', 'bgr', 'bicycle', 'big', 'bigger', 'biggest', 'bike', 'billboard', 'billion', 'bin', 'bing', 'biomimicry', 'bird', 'birds', 'birth', 'bit', 'bite', 'biz', 'bizzy', 'bjdproductions', 'black', 'blackberry', 'blackbook', 'blacked', 'blame', 'blast', 'bldg', 'bleed', 'blew', 'blind', 'blinksale', 'block', 'blocked', 'blocks', 'blog', 'bloggable', 'blogger', 'blogging', 'blogs', 'bloody', 'bloomberg', 'blowing', 'blows', 'blue', 'blueray', 'bluetooth', 'bluezoom', 'bmm', 'bnet', 'board', 'boarded', 'body', 'bomb', 'bonus', 'boo', 'book', 'bookbook', 'books', 'boom', 'boomers', 'boooo', 'boost', 'booth', 'boots', 'booyah', 'booze', 'borderstylo', 'bored', 'boring', 'born', 'borrow', 'borrowing', 'boss', 'botch', 'both', 'bother', 'bots', 'bottom', 'bought', 'bounced', 'bound', 'boundaries', 'bout', 'bowl', 'box', 'boxee', 'boxes', 'boy', 'boyfriend', 'boyfriends', 'boys', 'bpm', 'bracket', 'brah', 'brain', 'brains', 'brainwashed', 'brand', 'branded', 'brands', 'bravo', 'brawls', 'brazil', 'breach', 'bread', 'break', 'breakdown', 'breakfast', 'breaking', 'breakout', 'breakthrough', 'breath', 'breathtaking', 'breeds', 'brian_lam', 'brick', 'bricklin', 'bridging', 'brief', 'bright', 'brightens', 'brightness', 'brilliance', 'brilliant', 'bring', 'bringing', 'brings', 'brisk', 'british', 'brits', 'brk', 'bro', 'broadcast', 'broadcastr', 'broadfeed', 'broken', 'brother', 'brought', 'browse', 'browser', 'browserwars', 'browsing', 'bruises', 'brushstroke', 'bryce', 'bt', 'btw', 'bubble', 'bucket', 'buffalo', 'bug', 'bugger', 'buggy', 'bugs', 'build', 'building', 'buildings', 'built', 'bull', 'bulletin', 'bullish', 'bummed', 'bummer', 'bump', 'bumped', 'bunch', 'burn', 'bursts', 'bus', 'busdev', 'business', 'businesses', 'busy', 'but', 'butt', 'button', 'buttons', 'butts', 'buy', 'buyers', 'buying', 'buys', 'buzz', 'buzzing', 'buzzmetrics', 'by', 'bynd', 'ca', 'cab', 'cabbies', 'cable', 'cables', 'cabs', 'cactus', 'cake', 'calendar', 'calhoun', 'california', 'call', 'callback', 'called', 'calls', 'calyp', 'cam', 'came', 'camera', 'cameras', 'campaigns', 'campbell', 'campus', 'can', 'canada', 'canadian', 'cancel', 'cannot', 'cant', 'canvas', 'capabilities', 'capitol', 'capped', 'capture', 'captured', 'car', 'caramel', 'carbon', 'card', 'cards', 'care', 'career', 'caring', 'carousel', 'carry', 'carrying', 'cart', 'cartel', 'cartoon', 'cartoonishly', 'case', 'cases', 'cash', 'cashmere', 'cashmore', 'cast', 'castle', 'casually', 'cat', 'catch', 'catching', 'catfight', 'catphysics', 'cattle', 'cause', 'caused', 'causing', 'cautiously', 'cbatsxsw', 'cc', 'cedar', 'celebrate', 'celebrating', 'celebs', 'cell', 'cellular', 'center', 'centers', 'central', 'centre', 'centric', 'cents', 'ceo', 'ceokidschat', 'cera', 'cerebellum', 'cerebral', 'certain', 'certificate', 'ces', 'chain', 'chair', 'chalked', 'challenged', 'challenges', 'champ', 'chance', 'chances', 'change', 'changed', 'changer', 'changes', 'changing', 'channel', 'channels', 'chaos', 'characters', 'charge', 'charged', 'charger', 'chargers', 'charges', 'chargin2diffphonesatonce', 'charging', 'charity', 'charles', 'charm', 'charts', 'chat', 'chatter', 'chatting', 'cheap', 'cheapen', 'cheaper', 'check', 'checked', 'checking', 'checkins', 'cheeky', 'cheer', 'cheers', 'cheese', 'chen', 'chevy', 'chevysmc', 'chevysxsw', 'chevytweethouse', 'chic', 'chief', 'childhood', 'chill', 'chilltab', 'china', 'chinese', 'chip', 'chk', 'chng', 'choice', 'chokes', 'choose', 'choplifter', 'choreography', 'chris', 'christian', 'christmas', 'chrome', 'chromeos', 'chronicling', 'chumps', 'chunky', 'cigarettes', 'cinema', 'circle', 'circles', 'circus', 'circusmash', 'cited', 'cites', 'city', 'ck', 'cks', 'claims', 'clarity', 'clark', 'class', 'classics', 'classiest', 'classy', 'cle', 'clean', 'clear', 'clearly', 'cleveland', 'clever', 'click', 'clicked', 'client', 'clients', 'climbing', 'clip', 'clipcon', 'clocks', 'close', 'closed', 'closely', 'closer', 'clothes', 'cloud', 'cloudapp', 'cloudsight', 'clumsily', 'cluster', 'cluttering', 'cm48', 'cmswire', 'cmty', 'cn', 'cnet', 'cnn', 'cnngrill', 'cnnmoney', 'cnnmoneysxsw', 'cnt', 'cntr', 'co', 'cobra', 'cocaine', 'cocky', 'cocoon', 'code', 'coded', 'coders', 'coding', 'coffee', 'cohen', 'coincide', 'coincides', 'coinsidence', 'cold', 'colin', 'collab', 'collect', 'collection', 'collective', 'collectively', 'color', 'colors', 'colour', 'com', 'combine', 'combines', 'comcom', 'come', 'comedy', 'comers', 'comes', 'comfort', 'comfortable', 'comic', 'coming', 'commandeered', 'comment', 'comments', 'common', 'comms', 'communal', 'communicate', 'communication', 'communications', 'community', 'comp', 'compact', 'compan', 'companies', 'company', 'compared', 'compatible', 'compels', 'compete', 'competition', 'competitor', 'competitors', 'compiling', 'complement', 'complete', 'completely', 'completes', 'complex', 'complicated', 'composed', 'computer', 'computers', 'computing', 'concentrate', 'concept', 'concert', 'concertgoers', 'concierge', 'condense', 'conf', 'conference', 'conferences', 'confession', 'confines', 'confirmed', 'conflagration', 'confusion', 'congrats', 'congratulation', 'congratulations', 'congress', 'connect', 'connected', 'connectedcar', 'connectedtv', 'connectivity', 'connects', 'conquered', 'consciously', 'consequences', 'considering', 'consistent', 'consistently', 'constant', 'consultation', 'consulting', 'consume', 'consumer', 'consumerist', 'consumerization', 'cont', 'contact', 'content', 'contentrules', 'contest', 'context', 'contextual', 'continual', 'continued', 'continues', 'continuous', 'continuum', 'control', 'controller', 'conv', 'convenient', 'conveniently', 'convention', 'conventions', 'converge', 'conversation', 'conversations', 'conversion', 'convience', 'convince', 'convinced', 'convore', 'cool', 'coolaid', 'cooler', 'coolest', 'coolhaus', 'coolness', 'coordinate', 'cope', 'copia', 'copper', 'cops', 'copy', 'cor', 'cord', 'cordless', 'cords', 'core', 'corner', 'coronasdk', 'corporate', 'corporation', 'corps', 'corralling', 'correct', 'correcting', 'corrupted', 'cost', 'costs', 'costume', 'couch', 'couchfan', 'cough', 'could', 'couldn', 'count', 'countering', 'counting', 'country', 'couple', 'coupons', 'course', 'courtesy', 'courtside', 'courtyard', 'cover', 'coverage', 'covered', 'covering', 'covet', 'cow', 'cowboy', 'coworkers', 'cpa', 'cr', 'crack', 'crackberry', 'crap', 'crapkit', 'crapped', 'crappy', 'craps', 'crash', 'crashed', 'crasher', 'crashes', 'crashing', 'crashy', 'crave', 'craving', 'craziness', 'crazy', 'crazyco', 'crazyfest', 'cream', 'create', 'created', 'creates', 'creating', 'creative', 'creatively', 'creativity', 'creator', 'creators', 'creatures', 'credit', 'creek', 'creeper', 'crew', 'crippling', 'crisis', 'critiques', 'cross', 'crossed', 'crossing', 'crowd', 'crowdbeacon', 'crowded', 'crowds', 'crowdsourcing', 'crowley', 'crunch', 'crushing', 'crushit', 'cruze', 'cry', 'csr', 'cstejas', 'csuitecsourcing', 'ctia', 'cult', 'culture', 'cunning', 'cup', 'cupcake', 'cupertino', 'cups', 'curated', 'curatedebate', 'curiosity', 'curious', 'current', 'curse', 'cursing', 'cursor', 'custom', 'custome', 'customer', 'customers', 'customizable', 'cut', 'cute', 'cuts', 'cutsies', 'cuz', 'cwc2011', 'cwebb', 'cynical', 'da', 'dah', 'dahl', 'dailies', 'daily', 'dairy', 'dali', 'damage', 'damm', 'dammit', 'damn', 'damon', 'dan', 'dance', 'dancing', 'dandy', 'danfung', 'dang', 'dangerous', 'dangling', 'daniel', 'danny', 'dark', 'darn', 'darryl', 'dashboard', 'dat', 'data', 'database', 'dataviz', 'date', 'dating', 'david', 'davis', 'dawdled', 'dawg', 'dawn', 'dawned', 'day', 'daylight', 'days', 'de', 'dead', 'deadline', 'deadly', 'deal', 'dealing', 'deals', 'dear', 'death', 'debating', 'debut', 'debuting', 'debuts', 'decade', 'decent', 'decide', 'decided', 'deciding', 'decision', 'deck', 'dedication', 'deep', 'deeper', 'def', 'default', 'deficit', 'define', 'defining', 'definitely', 'deforestation', 'degrees', 'dehumanizing', 'delay', 'delayed', 'delegates', 'delete', 'deleting', 'delicious', 'deliciously', 'deliciousness', 'delight', 'delightful', 'delivered', 'delivering', 'delivery', 'dell', 'delving', 'demand', 'demo', 'democracy', 'demoed', 'demoing', 'demonstrate', 'demonstrates', 'demonstration', 'demos', 'denies', 'dennis', 'denotes', 'dense', 'density', 'depeche', 'depressed', 'described', 'design', 'designed', 'designers', 'designflaws', 'designing', 'designingforkids', 'desk', 'desktop', 'desktops', 'desperate', 'desperately', 'despite', 'destroyed', 'detail', 'detailed', 'details', 'detect', 'detection', 'dev', 'develop', 'developed', 'developer', 'developers', 'developing', 'development', 'deviantart', 'device', 'devices', 'devs', 'dexteria', 'dfp', 'dfw', 'dgtltribe', 'diabetes', 'dictaphone', 'dictators', 'dictatorship', 'did', 'didn', 'die', 'died', 'diego', 'dieing', 'dies', 'diet', 'diferencia', 'diff', 'difference', 'different', 'dig', 'digg', 'digging', 'digibiz', 'digital', 'digitalluxury', 'digitally', 'dilemma', 'diller', 'dimensional', 'dine', 'dinner', 'direct', 'direction', 'directions', 'director', 'directors', 'dirty', 'disabilities', 'disagree', 'disappointed', 'disappointingly', 'disaster', 'disc', 'discontinued', 'discotalk', 'discover', 'discovered', 'discovery', 'discovr', 'discuss', 'discusses', 'discussion', 'disgraceful', 'disgusted', 'disk', 'dislike', 'disliking', 'disney', 'disneyland', 'display', 'displaying', 'disrupt', 'disruptive', 'disrupts', 'distance', 'distract', 'distribution', 'disturbing', 'ditch', 'divasanddorks', 'divide', 'dividends', 'dj', 'djroe', 'dk', 'dl', 'dm', 'do', 'doc', 'dock', 'docomo', 'docs', 'documented', 'dodgeball', 'dodo', 'does', 'doesdroid', 'doesn', 'dog', 'dogs', 'doing', 'doingitwrong', 'dokobots', 'dollar', 'dollars', 'dom', 'domain', 'dominance', 'domo', 'don', 'donate', 'donates', 'donating', 'donation', 'done', 'dongle', 'donline', 'dont', 'dontbehatin', 'doo', 'doodle', 'doodles', 'doofusness', 'door', 'dorkinout', 'dotco', 'double', 'doubly', 'doubt', 'douche', 'douchebag', 'douchebaggery', 'dow', 'down', 'downer', 'download', 'downloaded', 'downloading', 'downloads', 'downside', 'downstairs', 'downtown', 'dr_black', 'drafthouse', 'drag', 'draining', 'draw', 'drawing', 'draws', 'dream', 'dreams', 'drink', 'drinks', 'drive', 'driven', 'drivers', 'drives', 'driving', 'droid', 'drooling', 'drop', 'dropped', 'dropping', 'drowning', 'drug', 'drumbeat', 'drumroll', 'drunk', 'drupalcon', 'dst', 'dt', 'dtas', 'ducks', 'dude', 'due', 'duh', 'duking', 'dumped', 'dunno', 'durable', 'during', 'dwindled', 'dwindling', 'dyac', 'dynamic', 'dynamics', 'ea1zgd', 'each', 'eagerly', 'earbud', 'earbuds', 'earlier', 'early', 'earned', 'earphones', 'earplugs', 'ears', 'earth', 'earthhour', 'earthquake', 'earths', 'easeljs', 'easier', 'easily', 'east', 'easy', 'eat', 'eatdrinktweet', 'eating', 'eats', 'ebay', 'ebooks', 'ecademy', 'ecko', 'economies', 'economy', 'ecosystem', 'ed', 'eddy', 'edible', 'edit', 'edition', 'edreform', 'edtech', 'education', 'educational', 'eduvc', 'eff', 'effective', 'efficient', 'effing', 'effort', 'efforts', 'eg', 'egomaniacs', 'eh', 'eightbit', 'either', 'elbow', 'election', 'electronics', 'elegant', 'elements', 'elevate', 'elevation', 'elonsxsw', 'else', 'email', 'emails', 'embarrassed', 'emc', 'eminent', 'emotional', 'employee', 'employees', 'empowered', 'empowering', 'empty', 'emulates', 'enable', 'enabled', 'enables', 'enchanted', 'enchanting', 'enchantment', 'end', 'endeavor', 'ended', 'endorsed', 'endorsement', 'ends', 'energy', 'engage365', 'engagement', 'engaging', 'engine', 'engineer', 'engines', 'english', 'enhancements', 'enjoy', 'enjoyed', 'enjoying', 'enjoys', 'enlightening', 'enough', 'ensue', 'ensues', 'enter', 'entered', 'enterprise', 'entertaining', 'entire', 'entirely', 'entrepreneur', 'entry', 'enuf', 'enviro', 'environment', 'environmental', 'envisioning', 'envy', 'epic', 'epicenter', 'epicurious', 'equates', 'equity', 'er', 'era', 'eric', 'error', 'ers', 'es', 'escape', 'esp', 'especially', 'essential', 'essentially', 'essentials', 'estate', 'et', 'etc', 'etch', 'etsbzk', 'etsy', 'euphoria', 'europe', 'eurosxsw', 'evade', 'evaporation', 'even', 'evening', 'event', 'eventbrite', 'eventprofs', 'events', 'eventseekr', 'eventually', 'ever', 'everbody', 'evernote', 'every', 'everybody', 'everyday', 'everyone', 'everything', 'everytime', 'everywhere', 'evidence', 'evil', 'evo', 'evolve', 'evolving', 'evolvingworkplace', 'ex', 'exactly', 'example', 'examples', 'excel', 'excellent', 'except', 'exceptionally', 'exchange', 'excited', 'excitement', 'exciting', 'excludes', 'exclusive', 'excuse', 'executed', 'executing', 'exhibit', 'exhibitors', 'exist', 'existence', 'existent', 'exists', 'exp', 'expect', 'expectation', 'expected', 'expecting', 'expensive', 'experience', 'experiential', 'experiment', 'experimenting', 'expert', 'experts', 'expierence', 'explaining', 'explanation', 'explode', 'exploiting', 'explorer', 'explorers', 'exploring', 'exported', 'exposing', 'express', 'exquisite', 'extended', 'extenders', 'extolling', 'extra', 'extraordinary', 'extras', 'extreme', 'eye', 'eyeballed', 'eyes', 'f6bcet', 'fab', 'fab5', 'fabulous', 'face', 'facebook', 'facepalmed', 'facetime', 'facing', 'facist', 'fact', 'factor', 'facts', 'fades', 'fail', 'failed', 'failing', 'failure', 'fair', 'fake', 'false', 'fam', 'familiarize', 'famous', 'fan', 'fanbois', 'fanboy', 'fanboyism', 'fanboys', 'fancrazed', 'fandango', 'fans', 'fantastic', 'fantastico', 'far', 'farm', 'farmers', 'farmville', 'farooqui', 'fascinating', 'fascist', 'fashion', 'fast', 'fastcompanygrill', 'fastcompanygrille', 'faster', 'fastest', 'fastsociety', 'fat', 'fathom', 'faulty', 'fav', 'fave', 'favorite', 'favorited', 'favorites', 'favour', 'fawning', 'fb', 'fear', 'feature', 'featured', 'features', 'featuring', 'feckin', 'fed', 'feed', 'feeding', 'feel', 'feelin', 'feeling', 'feelings', 'feels', 'fees', 'felix', 'fell', 'fellow', 'fellowship', 'felt', 'ferris', 'ferriss', 'fessing', 'fest', 'festgoers', 'festival', 'festivalgenius', 'fetishism', 'few', 'ff', 'fh', 'fi', 'field', 'fiendishly', 'fight', 'fighting', 'fightthepaddle', 'figure', 'figuring', 'fill', 'filled', 'filling', 'film', 'filmaster', 'filming', 'filter', 'filters', 'fin', 'final', 'finalist', 'finalists', 'finally', 'finals', 'find', 'finder', 'finding', 'finds', 'fine', 'finger', 'fingerprint', 'fingers', 'finish', 'finished', 'fire', 'firm', 'first', 'firstworldproblems', 'fishing', 'fists', 'fit', 'fits', 'five', 'fives', 'fiving', 'fix', 'fixing', 'flaming', 'flannel', 'flap', 'flash', 'flask', 'flavor', 'flaw', 'flawless', 'fleets', 'flew', 'flight', 'flights', 'flip', 'flipboard', 'floating', 'flocking', 'flood', 'floor', 'flop', 'florian', 'flow', 'fludapp', 'fluffertrax', 'fluid', 'flummoxed', 'fly', 'flying', 'flypost', 'fml', 'fmsignal', 'focus', 'focuses', 'focusing', 'fodder', 'foing', 'folks', 'follow', 'followed', 'followers', 'following', 'followings', 'fond', 'fondling', 'fonts', 'food', 'food4thought', 'foodies', 'foods', 'foodspotting', 'fools', 'foosspotting', 'foot', 'footage', 'footnotes', 'for', 'forbes', 'forbidden', 'force', 'forecast', 'forests', 'forever', 'forget', 'forgo', 'forgot', 'forgotten', 'form', 'format', 'formation', 'former', 'formerly', 'formula', 'forward', 'foster', 'found', 'four', 'foursquare', 'fr', 'fragmentation', 'framework', 'franchised', 'francisco', 'franco', 'frank', 'franken', 'frankeninterface', 'freak', 'freaking', 'free', 'freemusic', 'freespeech', 'freeze', 'frenzy', 'fresh', 'fri', 'frickin', 'fricking', 'friday', 'friend', 'friendly', 'friends', 'frm', 'from', 'front', 'frontend', 'frood', 'frostwire', 'frothy', 'frozen', 'fruit', 'frustrated', 'frustrating', 'frustration', 'ft', 'ftp', 'ftw', 'fuck', 'fucking', 'fuckit', 'fuckyeah', 'fuel', 'full', 'fulltime', 'fully', 'fun', 'function', 'functionality', 'fundraising', 'funny', 'fusion', 'future', 'futurecast', 'futuremf', 'futureoftouch', 'fwd', 'fxsw', 'fyi', 'g03mzb', 'g2', 'g4gzypv', 'gabacustweets', 'gadget', 'gadgetenvy', 'gadgets', 'gadgetzilla', 'gah', 'gain', 'gaining', 'gains', 'galaxy', 'gallery', 'galore', 'game', 'gamechanger', 'gamelayer', 'games', 'gamesfortv', 'gamestorming', 'gaming', 'gap', 'garageband', 'garyvee', 'gas', 'gasps', 'gatekeeper', 'gathering', 'gave', 'gawking', 'gay', 'gayno', 'gb', 'ge', 'gear', 'gecko', 'gee', 'geek', 'geekdate', 'geekdilemma', 'geekdom', 'geekery', 'geekest', 'geekfest', 'geeking', 'geekout', 'geeks', 'geeksrule', 'geeky', 'gen', 'general', 'generally', 'generated', 'generations', 'generous', 'genius', 'geniuses', 'gents', 'geo', 'geogames', 'geolocation', 'germ', 'gesture', 'get', 'getjarsxsw', 'gets', 'getting', 'gettinng', 'ghost', 'giant', 'gibson', 'giddy', 'gift', 'gig', 'giggle', 'giggling', 'gigs', 'gilt', 'ginger', 'girl', 'girlcrush', 'girls', 'gitchococktailon', 'give', 'giveaway', 'giveaways', 'given', 'gives', 'giving', 'glad', 'glass', 'glasses', 'glenda', 'glimpse', 'global', 'globalbestaward', 'globally', 'glow', 'glowing', 'glued', 'gmail', 'go', 'go2', 'god', 'goddamn', 'godsend', 'goer', 'goers', 'goes', 'gogglers', 'goggles', 'gogo', 'goin', 'going', 'gold', 'golden', 'golds', 'gone', 'gonna', 'gonnagetanipad2', 'goo', 'good', 'goodcustomerservice', 'gooddeed', 'goodguide', 'goodness', 'goody', 'goog', 'google', 'googleblog', 'googlebread', 'googlecircles', 'googled', 'googledoodle', 'googledoodles', 'googlegays', 'googleio', 'googlemaps', 'googleplaces', 'googletv', 'goona', 'gorgeous', 'got', 'gotta', 'gotten', 'gotto', 'government', 'govt', 'gowalla', 'gps', 'gr2l2', 'gr8', 'grab', 'grabbed', 'grabs', 'gram', 'gran', 'grand', 'grant', 'granted', 'grape', 'graph', 'graphic', 'graphics', 'grateful', 'gratification', 'great', 'greater', 'greatergood', 'greatest', 'greet', 'greeted', 'grew', 'griddler', 'griffin', 'grill', 'grindr', 'grn7pk', 'grooving', 'grossed', 'ground', 'groundlink', 'group', 'groupchatapps', 'groupme', 'groupon', 'groups', 'grow', 'growing', 'grown', 'grrr', 'grrrr', 'gruber', 'grumbling', 'gsd', 'gsdm', 'gswsxsw', 'gt', 'gtd', 'guard', 'guardian', 'guards', 'guess', 'guessing', 'guest', 'guguchu', 'guide', 'guidelines', 'guides', 'guild', 'guilty', 'guitar', 'gun', 'guru', 'guy', 'guykawasaki', 'guys', 'gvlrin', 'gym', 'h264', 'h2o', 'h4ckers', 'ha', 'hack', 'hackathon', 'hacker', 'hackers', 'hacknews', 'had', 'hah', 'haha', 'hahaha', 'hair', 'hairy', 'haiti', 'half', 'halfway', 'hall', 'halls', 'hallway', 'hamsandwich', 'hand', 'handicapped', 'handing', 'handle', 'handled', 'hands', 'handset', 'handsome', 'handwriting', 'handy', 'hang', 'hanging', 'hangover', 'hangover3', 'hannukah', 'happen', 'happened', 'happening', 'happiest', 'happily', 'happy', 'happydance', 'hard', 'harlow', 'harnessing', 'harris', 'has', 'hash', 'hashable', 'hashtag', 'hashtags', 'hasn', 'hassle', 'hate', 'hated', 'haul', 'hauling', 'have', 'haven', 'havent', 'having', 'havnt', 'hawk', 'hawt', 'haystack', 'haz', 'hcsm', 'hdmi', 'he', 'head', 'headaches', 'headed', 'heading', 'headline', 'headphones', 'heads', 'headsets', 'health2dev', 'hear', 'heard', 'hearing', 'heart', 'heat', 'heating', 'heatmap', 'heats', 'heattracker', 'heavenly', 'heavens', 'heavier', 'heaving', 'heavy', 'heck', 'held', 'hell', 'hello', 'help', 'helped', 'helpful', 'helping', 'helps', 'her', 'here', 'hereforwork', 'herself', 'hey', 'heyo', 'hhaha', 'hhrs', 'hi', 'hidden', 'hide', 'high', 'higher', 'highlight', 'highlights', 'highly', 'highs', 'hijack', 'hilarious', 'hill', 'hilton', 'him', 'hint', 'hints', 'hip', 'hipstamatic', 'hipstapaks', 'hipster', 'hipstermuch', 'hipsters', 'hire', 'hireme', 'hirer', 'his', 'hiss', 'history', 'hisxsw', 'hit', 'hitlantis', 'hive', 'hls', 'hm', 'hmm', 'hmmm', 'hmmzies', 'hobo', 'hoc', 'hoffman', 'hold', 'holding', 'hole', 'holla', 'holler', 'hollergram', 'hollow', 'hollrback', 'hollywood', 'holy', 'holytrafficjams', 'home', 'homeless', 'homepage', 'homogeneity', 'honesty', 'honor', 'honors', 'hoo', 'hook', 'hooked', 'hoooooooooooooo', 'hooray', 'hoot', 'hooting', 'hootsuite', 'hop', 'hope', 'hopefully', 'hopes', 'hoping', 'hopkins', 'hordes', 'horrendous', 'horrible', 'horror', 'host', 'hosted', 'hosting', 'hot', 'hotel', 'hotels', 'hotpot', 'hotspot', 'hottest', 'hour', 'hours', 'house', 'housecat', 'housing', 'how', 'howdy', 'howmto', 'hp', 'hpsxsw', 'hr', 'hrs', 'ht', 'htdfim', 'html', 'html5', 'http', 'https', 'hubby', 'huge', 'human', 'hundred', 'hundreds', 'hunger', 'hungry', 'hunt', 'hunts', 'hurricaneparty', 'husband', 'huzzah', 'hyatt', 'hype', 'hyped', 'i41h53', 'ia', 'iads', 'ical', 'icanhas', 'ice', 'icebreaker', 'iconbuffet', 'id', 'id420666439', 'idea', 'ideally', 'ideas', 'identity', 'idiocy', 'idiot', 'idk', 'idol', 'idontbelieve', 'ie', 'ie9', 'ieavob', 'if', 'ifr3dw', 'ifrom', 'ignite', 'ignore', 'igottagetit', 'ihop', 'il', 'illa', 'illegal', 'illmakeitwork', 'iloveasurprise', 'im', 'imac', 'imaconf', 'imacs', 'image', 'images', 'imagine', 'imanidiot', 'imanoutcast', 'imho', 'immersive', 'immobile', 'imo', 'imp1000', 'impact', 'impactdashboard', 'impediment', 'impedimenta', 'implement', 'implementation', 'implementing', 'implode', 'important', 'impossible', 'impressed', 'impression', 'impressions', 'impressive', 'impromptu', 'improve', 'improvement', 'improvements', 'improvemnt', 'impulse', 'impulsive', 'imrich', 'imthatgood', 'in', 'inane', 'inbox', 'incapable', 'incenticize', 'incl', 'include', 'included', 'includes', 'including', 'incorrect', 'increase', 'incredible', 'incredibly', 'indeed', 'indicates', 'indie', 'indigenous', 'individuals', 'indoor', 'industry', 'infact', 'infektd', 'inferior', 'influence', 'influencers', 'influx', 'info', 'informal', 'information', 'informed', 'ing', 'ingenious', 'initial', 'initiative', 'ink', 'innacurate', 'inner', 'innotribe', 'innovate', 'innovating', 'innovation', 'innovative', 'innovators', 'input', 'ins', 'insane', 'insanely', 'insatiable', 'insertion', 'inside', 'insider', 'insidious', 'insight', 'insightful', 'insights', 'insists', 'inspired', 'inspiring', 'instagram', 'install', 'installed', 'installing', 'instant', 'instantly', 'instead', 'instruments', 'integrated', 'integration', 'intel', 'intelligence', 'intelligent', 'intended', 'intense', 'interact', 'interactive', 'interested', 'interesting', 'interface', 'interfaces', 'intermittent', 'international', 'internet', 'internetonlinewebsite', 'interrupt', 'interview', 'interviewed', 'intimate', 'intimidated', 'into', 'intrestin', 'intricate', 'intriguing', 'intro', 'introduced', 'introduces', 'introducing', 'intrvw', 'invades', 'inventing', 'inventory', 'invest', 'investment', 'investor', 'investors', 'invisible', 'invite', 'invited', 'invites', 'invoking', 'io', 'ios', 'ip', 'ip4', 'ipad', 'ipad1', 'ipad2', 'ipad2s', 'ipad2time', 'ipad_2', 'ipaddesignheadaches', 'ipading', 'ipadmadness', 'ipads', 'iphone', 'iphone4', 'iphone5', 'iphones', 'ipod', 'ipods', 'ipoo', 'iqlab', 'iradar', 'irelay', 'ireport', 'ireports', 'iron', 'ironic', 'irrelevant', 'irresistible', 'is', 'isack', 'ischafer', 'isn', 'issue', 'issues', 'istache', 'istock', 'it', 'item', 'itme', 'its', 'itself', 'ittttt', 'itun', 'itunes', 'itwillbemine', 'iusxsw', 'iwantacameraonmyipad', 'ixd', 'jailbreak', 'jaloux', 'james', 'jammy', 'janecek', 'japan', 'japanese', 'jared', 'java', 'javascript', 'jaw', 'jcpenney', 'jealous', 'jeanne', 'jeans', 'jeebus', 'jeez', 'jeff', 'jerk', 'jerranalley', 'jessedee', 'jesus', 'jet', 'jetlag', 'jetsons', 'jinx', 'jk', 'jo', 'job', 'jobs', 'jobs_co', 'jobsco', 'joe', 'johnston', 'join', 'joined', 'joins', 'joint', 'joke', 'jonathan', 'joomla', 'jose', 'josh', 'journalists', 'journalsim', 'journey', 'joy', 'jpmobilesummit', 'jqtouch', 'jr', 'js', 'judging', 'juice', 'juiced', 'juicepack', 'julian', 'julie', 'jump', 'june', 'just', 'justin', 'justinjustinjustin', 'justmet', 'justsayin', 'justsaying', 'juts', 'juxtaposed', 'jwtatl', 'jzsxsw', 'kara', 'karaoke', 'karateka', 'kawasaki', 'keep', 'keepaustinweird', 'keeping', 'keeps', 'keg', 'kek', 'ken', 'kenny', 'ketchsx', 'keyboard', 'keynote', 'keys', 'keywords', 'kh', 'khan', 'khoi', 'kick', 'kickass', 'kicked', 'kicking', 'kid', 'kiddie', 'kidding', 'kids', 'kiiiiiilling', 'kik', 'kill', 'killcommunity', 'killer', 'killers', 'killing', 'kind', 'kinda', 'kindle', 'king', 'kingston', 'kiosk', 'kirkus', 'kiss', 'kit', 'klick', 'klm', 'knackered', 'knew', 'knickers', 'knife', 'kno', 'knockout', 'know', 'knowing', 'knowledge', 'known', 'knows', 'korean', 'korine', 'kthxbai', 'kudos', 'kweli', 'kyping', 'la', 'lab', 'labs', 'lack', 'ladies', 'lady', 'lake', 'lame', 'lamesauce', 'land', 'landlords', 'landmark', 'landscapes', 'lanyrd', 'lap', 'laptop', 'laptops', 'large', 'larger', 'largest', 'larry', 'laser', 'last', 'lasts', 'late', 'lately', 'later', 'latest', 'latitude', 'laugh', 'laughed', 'launch', 'launched', 'launches', 'launching', 'launchrock', 'laurieshook', 'lava', 'lavelle', 'law', 'laws', 'lax', 'layer', 'lazy', 'lazyweb', 'lbs', 'lead', 'leading', 'league', 'leaked', 'leaning', 'leanstartup', 'learn', 'learned', 'learning', 'learnt', 'leash', 'leasing', 'least', 'leather', 'leave', 'leaves', 'leaving', 'left', 'legacy', 'lego', 'leisure', 'leisurely', 'lemon', 'length', 'leo', 'leopard', 'less', 'lessons', 'let', 'lets', 'letschangetheworld', 'letter', 'letters', 'letting', 'letushopenot', 'level', 'leveraging', 'lewis', 'liberty', 'libraries', 'library', 'licked', 'lie', 'life', 'lifeless', 'lifelinetotheworld', 'lifetime', 'light', 'lightbox', 'lightbox_photos', 'lightbulb', 'lighters', 'likability', 'like', 'likeability', 'liked', 'likely', 'likes', 'liking', 'lil', 'limit', 'limited', 'limp', 'lindsay', 'line', 'lines', 'lineup', 'lining', 'link', 'linking', 'links', 'lions', 'lisa', 'list', 'listen', 'listened', 'listening', 'lists', 'literally', 'litle', 'little', 'live', 'liveblog', 'lives', 'livesteam', 'livetapp', 'living', 'livingthedream', 'll', 'lmndst', 'load', 'loaded', 'loading', 'loathe', 'lobby', 'lobbying', 'local', 'localmind', 'locals', 'locating', 'location', 'locations', 'lockers', 'lockout', 'logic', 'logical', 'login', 'logo', 'logos', 'lol', 'lonely', 'lonelyplanet', 'long', 'longer', 'longlinesbadux', 'look', 'lookalike', 'looked', 'lookin', 'looking', 'lookingforwardtothemusicfest', 'looks', 'loose', 'looseorganizations', 'lord', 'lordy', 'lorry', 'lose', 'losers', 'losing', 'lost', 'lot', 'lots', 'lottery', 'loud', 'louis', 'louisiana', 'lounge', 'lousy', 'love', 'loved', 'lovefresh', 'loveher', 'lovely', 'lovemusicapi', 'lovers', 'loves', 'lovesit', 'lovin', 'loving', 'lowest', 'loyalists', 'loyalty', 'lp', 'lt', 'luck', 'luckily', 'lucky', 'ludicon', 'lug', 'lugging', 'lunch', 'lust', 'lustre', 'luxury', 'lxh', 'ly', 'lybian', 'lying', 'lynn', 'ma', 'mac', 'macallan', 'macbook', 'macbookpro', 'macchiato', 'machine', 'machines', 'mackbook', 'macs', 'macys', 'mad', 'made', 'madebymany', 'madness', 'mae', 'maes', 'magazine', 'magazines', 'maggie', 'magic', 'maglight', 'magnet', 'magnetic', 'magnifying', 'mags', 'mail', 'major', 'majority', 'make', 'makery', 'makes', 'makeshift', 'making', 'malady', 'malbonster', 'mall', 'malt', 'man', 'manage', 'management', 'manager', 'managing', 'mania', 'manor', 'mantra', 'many', 'map', 'mapped', 'mappers', 'mapping', 'mapquest', 'maps', 'mar', 'marc', 'marcelosomers', 'march', 'margarita', 'margin', 'marisa', 'marissa', 'marissagoogle', 'marissamayer', 'marissameyer', 'mark', 'marker', 'market', 'marketer', 'marketers', 'marketing', 'marketplace', 'markets', 'mart', 'martinis', 'marty', 'marys', 'masha', 'mashable', 'mashbash', 'mass', 'masses', 'massive', 'mastered', 'matching', 'mater', 'math', 'matt', 'matter', 'matthew', 'maudies', 'mavis', 'maximum', 'may', 'maybe', 'mayer', 'mayers', 'mayor', 'mb', 'mbp', 'mccannsxsw', 'mdw', 'me', 'mealtime', 'mean', 'meaning', 'meaningful', 'means', 'meant', 'measurement', 'measuring', 'meat', 'mecca', 'mechanics', 'media', 'meet', 'meeti', 'meeting', 'meetings', 'meets', 'meetup', 'meetups', 'mega', 'megastore', 'meh', 'mekong', 'mel', 'member', 'memolane', 'memories', 'men', 'mental', 'mention', 'mentioned', 'mentionn', 'mentionr', 'mercy', 'message', 'messages', 'messaging', 'messed', 'messenger', 'messina', 'met', 'metaphor', 'methinks', 'mexican', 'mexico', 'mhealth', 'miamibeach', 'mic', 'michael', 'michaelpiliero', 'microformats', 'microsoft', 'mid', 'midday', 'middle', 'midem', 'midnight', 'midst', 'midway', 'mifi', 'might', 'mike', 'miles', 'military', 'mill', 'miller', 'million', 'millions', 'min', 'mind', 'minded', 'mindjet', 'mindmanager', 'minds', 'mindshare', 'mindtouch', 'mindtouchers', 'mine', 'ming', 'mini', 'minimalistprogramming', 'minor', 'mins', 'mint', 'minute', 'minutes', 'miracle', 'mirroring', 'miss', 'missed', 'missing', 'mission', 'misstatements', 'mistake', 'mistakes', 'mister', 'mitharvard', 'mix', 'mixed', 'mixing', 'mk', 'mkesxsw', 'mkt', 'mktg', 'mmm', 'mmod', 'mnbuzz', 'mngr', 'mobil', 'mobile', 'mobilefarm', 'mobileroadie', 'mobs', 'mock', 'mocked', 'mocking', 'mode', 'model', 'models', 'moderator', 'mojo', 'mom', 'moma', 'moment', 'moments', 'mommy', 'mon', 'monday', 'mondays', 'monetization', 'money', 'monitor', 'monopoly', 'monster', 'month', 'monthly', 'months', 'mood', 'moody', 'moonbot', 'moonshine', 'mophie', 'more', 'moreknowledge', 'morning', 'morphie', 'mosaicxm', 'most', 'mostly', 'motherboard', 'mothers', 'motivator', 'motorola', 'mountain', 'mounts', 'mouse', 'move', 'movement', 'movers', 'moves', 'movie', 'movies', 'moving', 'mozilla', 'mp', 'mp3', 'mq', 'mr', 'mrs', 'msft', 'mt', 'much', 'mullenweg', 'multiple', 'muro', 'murphy', 'museum', 'museums', 'music', 'musicians', 'musicviz', 'must', 'mute', 'muting', 'mwrc11', 'mxm', 'my', 'myegc', 'mylunch', 'mypov', 'myself', 'mystery', 'nab', 'nah', 'nailed', 'naive', 'name', 'named', 'naomi', 'nat', 'native', 'natural', 'navigating', 'navigation', 'nba', 'nc', 'ncaa', 'ncf', 'near', 'nearly', 'neat', 'need', 'needle', 'needs', 'neither', 'nerd', 'nerdbird', 'nerdcore', 'nerdheaven', 'nerdiest', 'nerds', 'nerdy', 'ness', 'net', 'netbook', 'netflix', 'netflixiphone', 'network', 'networking', 'networks', 'neumann', 'never', 'nevertheless', 'new', 'newapplestoreaustin', 'newest', 'newly', 'news', 'newsapp', 'newsapps', 'newspaper', 'newspapers', 'newsworthy', 'newtrent', 'next', 'nextflix', 'nexus', 'nfc', 'nfl', 'nfusion', 'nhk', 'nice', 'nicely', 'niceness', 'nick', 'nieuwe', 'nifty', 'night', 'nightjar', 'nike', 'nine', 'nineties', 'ning', 'ninja', 'ninjafinder', 'no', 'nobody', 'noes', 'nokia', 'nokiaconnects', 'non', 'nonprofit', 'nonprofits', 'noon', 'nope', 'nor', 'normal', 'north', 'not', 'notch', 'note', 'notes', 'notetaker', 'notevenstartedyet', 'nothing', 'notice', 'noticed', 'notionink', 'notpouting', 'notsomuch', 'nottheipad2', 'novelty', 'novideo', 'now', 'nowhammies', 'npr', 'nptech', 'nten', 'ntn', 'nuances', 'nudgenudge', 'numbassonfloor', 'number', 'nuts', 'nutshell', 'nutters', 'nvidia', 'nxt', 'nyc', 'nyt', 'object', 'objective', 'obs', 'observation', 'observations', 'observer', 'obsessed', 'obsolete', 'obv', 'obvious', 'obviously', 'occasional', 'of', 'off', 'offer', 'offered', 'offering', 'offers', 'office', 'officer', 'official', 'officially', 'offline', 'offsite', 'often', 'ogilvy', 'ogilvynotes', 'oh', 'oil', 'ok', 'okay', 'ol', 'old', 'oldschool', 'oldsko0l', 'omaha', 'omfg', 'omg', 'omgz', 'omitting', 'on', 'once', 'one', 'ones', 'online', 'only', 'onto', 'ooing', 'oooh', 'ooooo', 'open', 'openbeta', 'openbeta6', 'opened', 'openexhibits', 'opening', 'opens', 'operators', 'opinions', 'opportunity', 'opposite', 'optimistic', 'optimized', 'optimum', 'options', 'optiscan', 'or', 'orange', 'order', 'ordered', 'ordering', 'ordinance', 'org', 'organic', 'organically', 'organization', 'organize', 'organized', 'organizing', 'original', 'orkut', 'orlando', 'orly', 'oscars', 'osmpw', 'ossum', 'other', 'others', 'otherwise', 'ouch', 'our', 'ours', 'out', 'outbrain', 'outdid', 'outlandish', 'outlet', 'outs', 'outside', 'over', 'overall', 'overblown', 'overcome', 'overflow', 'overheard', 'overheating', 'overlaid', 'overlapping', 'overlay', 'overload', 'overshadowing', 'oversized', 'overtaken', 'overview', 'overwhelming', 'owl', 'owllove', 'own', 'owner', 'owners', 'owns', 'oy', 'pac', 'pacific', 'pack', 'packed', 'packing', 'packrat', 'packs', 'pacman', 'padless', 'page', 'pagemaker', 'pages', 'paid', 'painful', 'pair', 'paired', 'pak', 'pakistan', 'palette', 'palsy', 'pandora', 'panel', 'panelist', 'panelists', 'panels', 'panic', 'panned', 'panning', 'panorama', 'pants', 'paolo', 'papa', 'papasangre', 'paper', 'paperless', 'papyrus', 'para', 'parachute', 'paradigms', 'parentheses', 'pariah', 'park', 'parked', 'part', 'partial', 'participants', 'participating', 'parties', 'partner', 'partnerhub', 'partnership', 'parts', 'party', 'partying', 'partytweets', 'passage', 'passed', 'passenger', 'passerby', 'passes', 'passing', 'passport', 'past', 'paste', 'patch', 'patented', 'path', 'patience', 'pauly', 'pause', 'pavement', 'pay', 'paying', 'payingwithdata', 'payments', 'paypal', 'pc', 'pcbuzz', 'pcma', 'pdanet', 'pdf', 'pdx', 'peaked', 'pearl', 'peddle', 'pedicab', 'peek', 'peeked', 'peeps', 'peer', 'pen', 'penalty', 'penetrates', 'people', 'pep', 'per', 'percent', 'percentage', 'perfect', 'perfectly', 'performance', 'perhaps', 'periscope', 'permanent', 'permanently', 'perserverance', 'person', 'personal', 'personalcloud', 'personalized', 'peter', 'petition', 'petricone', 'petting', 'pg', 'pgi', 'phenomenal', 'phew', 'phone', 'phones', 'photo', 'photobooth', 'photoes', 'photos', 'photosharing', 'physical', 'pi', 'pic', 'pick', 'picked', 'picking', 'pickmeupanipad2', 'pics', 'picture', 'pictures', 'pie', 'piece', 'pig', 'pigfucker', 'pile', 'pilhofer', 'pillow', 'pink', 'pinoy', 'piping', 'piss', 'pissedimnotgoingtosxsw', 'pissing', 'pit', 'pitch', 'pitchforks', 'pitfalls', 'pitted', 'pix', 'pixel', 'pixels', 'pixieengine', 'place', 'places', 'plaid', 'plain', 'plan', 'plancast', 'plane', 'planely', 'planes', 'planet', 'planner', 'planning', 'plans', 'planting', 'planzai', 'plate', 'platform', 'play', 'playback', 'playbook', 'played', 'player', 'players', 'playhopskoch', 'playing', 'plays', 'playstation', 'playsxsw', 'please', 'pleased', 'plenty', 'plied', 'plixi', 'pls', 'plug', 'pluged', 'plugin', 'plunge', 'plus', 'pm', 'pnid', 'pocket', 'podcast', 'poetry', 'point', 'pointer', 'points', 'police', 'policy', 'political', 'politics', 'pollak', 'ponies', 'poo', 'pool', 'poole', 'poor', 'poos', 'pop', 'popped', 'popping', 'popplet', 'poppop', 'popular', 'populous', 'popup', 'popupshop', 'popupstore', 'por', 'portable', 'porting', 'posed', 'position', 'positioning', 'positive', 'positively', 'positives', 'possibility', 'possible', 'possibly', 'post', 'posted', 'posterous', 'postpc', 'pot', 'potential', 'potentially', 'pour', 'poursite', 'power', 'powered', 'powerful', 'powerhouse', 'powering', 'powermat', 'powermatteam', 'powermattteam', 'pp', 'ppl', 'pr', 'practice', 'pragmatic', 'pre', 'precedent', 'precommerce', 'predictability', 'prefer', 'preference', 'preferences', 'preferrably', 'preferred', 'prefers', 'premature', 'premiere', 'premium', 'prep', 'preparation', 'preparations', 'prepared', 'preparing', 'prepping', 'presence', 'present', 'presentation', 'presented', 'presenters', 'presenting', 'preso', 'presos', 'press', 'pressie', 'pressure', 'pretty', 'prettycool', 'preview', 'previews', 'previous', 'price', 'prices', 'pride', 'princess', 'principles', 'print', 'printed', 'prints', 'priorities', 'priority', 'privacy', 'private', 'prize', 'prizes', 'pro', 'prob', 'probably', 'problem', 'process', 'prodmktg', 'produced', 'producers', 'product', 'products', 'professionals', 'profile', 'profits', 'program', 'programming', 'progressbar', 'progression', 'project', 'project314', 'projecting', 'projects', 'promises', 'promo', 'promote', 'promotion', 'prompt', 'proof', 'propping', 'proprietary', 'props', 'protect', 'protecting', 'protip', 'protocol', 'proud', 'proven', 'provide', 'provided', 'providing', 'proving', 'prx', 'ps', 'pseudoretweet', 'psfk', 'pst', 'psych', 'psyched', 'psyches', 'pub', 'pubcamp', 'public', 'publishers', 'publishing', 'pubs', 'pull', 'pulling', 'pumped', 'pumps', 'pun', 'puppy', 'purchase', 'purchased', 'purchasers', 'purchases', 'purchasing', 'pure', 'puregenius', 'purpose', 'push', 'pushed', 'pusher', 'pushio', 'pushsnowboarding', 'put', 'puts', 'putting', 'puzzles', 'q7a', 'qa', 'qagb', 'qho', 'qr', 'qrafter', 'qrank', 'qrcode', 'qs', 'quadroid', 'quake', 'qualcomm', 'qualified', 'quality', 'quantity', 'quantter', 'quarantined', 'quarter', 'quarters', 'que', 'queries', 'question', 'questioner', 'questions', 'queue', 'quibids', 'quibidswin', 'quick', 'quicker', 'quickly', 'quiet', 'quinn', 'quit', 'quite', 'quot', 'quotables', 'quotes', 'race', 'rachael', 'rad', 'radian6', 'radical', 'radio', 'raffled', 'raffling', 'rage', 'rain', 'rainjacket', 'raised', 'raises', 'rallying', 'ran', 'rana', 'random', 'randomly', 'randy', 'range', 'rank', 'ranked', 'ranking', 'rankings', 'rant', 'rare', 'rate', 'rates', 'rather', 'rating', 'ratings', 'ratio', 'ray', 're', 'reach', 'reached', 'reacquainted', 'read', 'reader', 'readership', 'reading', 'ready', 'real', 'realistic', 'reality', 'realize', 'realized', 'realizing', 'really', 'realtalk', 'realtime', 'rear', 'reason', 'reasonable', 'reasoning', 'reasons', 'rebecca', 'rebels', 'rebeltv', 'rebranded', 'recap', 'received', 'recharge', 'recharging', 'recipe', 'recipes', 'recipient', 'reclaimed', 'recognition', 'recognize', 'recognizing', 'recomds', 'recommend', 'recommendation', 'recommendations', 'recommended', 'recommends', 'record', 'recorder', 'recording', 'recos', 'recovery', 'recreated', 'recs', 'recycled', 'red', 'redbox', 'redbull', 'redbullbpm', 'reddit', 'rediculous', 'reel', 'reeling', 'ref', 'referrals', 'refine', 'refrigerator', 'regarded', 'regel', 'regions', 'register', 'registers', 'registrant', 'registration', 'regrets', 'regretting', 'regular', 'regularly', 'rei', 'reid', 'reilly', 'reily', 'rejection', 'related', 'relation', 'relationship', 'relaxed', 'relaxing', 'release', 'released', 'releases', 'releasing', 'relevance', 'relevant', 'relief', 'relies', 'religion', 'relinquish', 'relive', 'reliving', 'rely', 'remaining', 'rematch', 'remedied', 'remember', 'remembered', 'reminding', 'removable', 'remove', 'rendering', 'renders', 'repair', 'replaced', 'replacement', 'replacing', 'replenished', 'replicate', 'replies', 'report', 'reporting', 'reports', 'repressed', 'reproducing', 'republic', 'reputation', 'rerouted', 'rescuing', 'research', 'resetting', 'resist', 'resonance', 'resource', 'resourceful', 'respect', 'respectfully', 'respecting', 'response', 'responses', 'responsibility', 'rest', 'restaurant', 'restaurants', 'restful', 'resting', 'restore', 'restored', 'restraunts', 'result', 'resulting', 'results', 'resume', 'retail', 'retiring', 'retrollect', 'return', 'retweet', 'retweeting', 'reveals', 'revelations', 'revenge', 'revenue', 'review', 'reviews', 'revolt', 'revolution', 'revolutionary', 'revolutions', 'reward', 'rewards', 'rewardswagon', 'rf', 'rfid', 'rhizome', 'richard', 'ridculous', 'ride', 'rides', 'ridic', 'ridicule', 'ridiculous', 'ridiculously', 'riding', 'rig', 'rigeur', 'right', 'rightfully', 'rim', 'rimmed', 'ringing', 'rinna', 'riots', 'rip', 'ripped', 'ripping', 'rise', 'rise_austin', 'rises', 'rite', 'river', 'rji', 'rm', 'road', 'roadie', 'roaming', 'robot', 'robots', 'rock', 'rockaroke', 'rocked', 'rockin', 'rocking', 'rocks', 'rockstache', 'role', 'roll', 'rolled', 'rollout', 'roof', 'room', 'rosso', 'rotational', 'round', 'route', 'routes', 'routing', 'row', 'rows', 'rpg', 'rsq', 'rsvp', 'rt', 'rub', 'rubbing', 'rule', 'rules', 'rumor', 'rumored', 'rumors', 'rumours', 'run', 'runaround', 'running', 'runs', 'rww', 'saatchiny', 'saber', 'sabotaged', 'sad', 'sadly', 'safari', 'said', 'sale', 'sales', 'salesperson', 'salon', 'sam', 'same', 'sampler', 'samsung', 'samsungmobileus', 'san', 'sandwich', 'sandwiched', 'sangre', 'sans', 'sapient', 'sat', 'satanic', 'saturday', 'sauce', 'save', 'savebrands', 'saved', 'saves', 'saveustechies', 'saving', 'savings', 'savvy', 'saw', 'say', 'sayin', 'saying', 'says', 'saysshewithoutanipad', 'scale', 'scan', 'scanner', 'scans', 'scarborough', 'scarbrough', 'scared', 'scarfing', 'scary', 'scavenger', 'scene', 'scenes', 'scepticism', 'sched', 'schedule', 'scheduled', 'scheduler', 'schedules', 'scheduling', 'schemas', 'school', 'schoolgirls', 'schools', 'schtuff', 'schwag', 'science', 'scientific', 'scoping', 'score', 'scoremore', 'scouts', 'screamed', 'screams', 'screen', 'screenfuture', 'screening', 'screenings', 'script', 'sd', 'sea', 'search', 'searchable', 'searches', 'searching', 'season', 'seat', 'seated', 'seats', 'seattle', 'sec', 'second', 'seconds', 'secret', 'security', 'see', 'seeing', 'seem', 'seemed', 'seemingly', 'seems', 'seen', 'seenocreepy', 'sees', 'seesmic', 'select', 'selected', 'selection', 'self', 'selfish', 'sell', 'selling', 'sells', 'sem', 'semantic', 'semi', 'semis', 'send', 'sending', 'sense', 'sent', 'seo', 'separate', 'september', 'serendipity', 'serious', 'seriously', 'seriousness', 'serv', 'serve', 'served', 'servers', 'serves', 'service', 'services', 'sesh', 'session', 'sessions', 'set', 'seta', 'sets', 'setting', 'settle', 'settling', 'setup', 'several', 'severe', 'severely', 'severity', 'sexy', 'sez', 'sfo', 'shade', 'shades', 'shakers', 'shakespeare', 'shall', 'shallow', 'shame', 'shamed', 'shameless', 'shaping', 'share', 'shareable', 'shared', 'sharers', 'sharing', 'sharp', 'shat', 'shatter', 'she', 'sheen', 'sheeple', 'shell', 'shelves', 'sheraton', 'shift', 'shill', 'shiner', 'shinmy', 'shiny', 'shipment', 'shipments', 'ships', 'shirt', 'shit', 'shitty', 'shocked', 'shoot', 'shooting', 'shop', 'shops', 'short', 'shortcuts', 'shortening', 'shortly', 'shot', 'shotgun', 'should', 'shoulda', 'shoulder', 'shouldn', 'shout', 'shouts', 'show', 'showcase', 'showcased', 'showcases', 'showcasing', 'showed', 'showing', 'shows', 'showusyouricrazy', 'shrink', 'shuffling', 'shut', 'sick', 'side', 'sides', 'sigh', 'sighting', 'sightings', 'sign', 'signal', 'signals', 'signed', 'signing', 'signs', 'silicon', 'sillier', 'silly', 'silver', 'simple', 'simplicity', 'simply', 'simultaneously', 'sin', 'since', 'singing', 'single', 'sings', 'singularity', 'sipping', 'sis', 'sister', 'sit', 'sitby', 'site', 'sites', 'sitting', 'six', 'sixth', 'size', 'skateboards', 'sketch', 'sketchy', 'skewed', 'skiers', 'skill', 'skillfully', 'skills', 'skinny', 'skip', 'skulls', 'skyfire', 'skynet', 'skype', 'slap', 'slated', 'sleek', 'sleep', 'sleepy', 'sleeves', 'slice', 'sliced', 'slick', 'slides', 'slightly', 'slim', 'slips', 'sloansxsw', 'sloanxsw', 'slow', 'slower', 'slowly', 'slowpoke', 'slp', 'smackdown', 'small', 'smallbiz', 'smaller', 'smart', 'smarter', 'smartest', 'smartphone', 'smartphones', 'smartthings', 'smarty', 'smashed', 'smcdallas', 'smcomedyfyeah', 'smell', 'smileyparty', 'smm', 'smmnextgen', 'smokes', 'smooth', 'sms', 'smtravel', 'smudgy', 'smugness', 'smurf', 'smut', 'smvis', 'smyle', 'sn', 'snagged', 'snakeheead', 'snap', 'snapping', 'snarky', 'sneakers', 'sneaky', 'snubor', 'so', 'social', 'socialfuel', 'socially', 'socialmedia', 'socialmediabum', 'socialmuse', 'socialviewing', 'society', 'socks', 'socmedia', 'socnet', 'softball', 'software', 'solar', 'sold', 'solely', 'solid', 'solo', 'solution', 'solutions', 'solving', 'some', 'somebody', 'someday', 'somehow', 'someone', 'someones', 'something', 'somewhere', 'song', 'songs', 'sonos', 'sony', 'soo', 'soon', 'sooo', 'sore', 'sorry', 'sort', 'sorta', 'sorted', 'soul', 'sound', 'soundcloud', 'sounding', 'sounds', 'soundtrckr', 'source', 'south', 'southby', 'southpaw', 'southwest', 'space', 'spanking', 'spark', 'spasmatics', 'spazmatic', 'spazmatics', 'spazzmatics', 'speak', 'speakeasy', 'speaking', 'speaks', 'special', 'specific', 'speech', 'speed', 'speedup', 'spell', 'speller', 'spend', 'spending', 'spent', 'spider', 'spilled', 'spiltbeer', 'spin', 'spinning', 'spins', 'spirit', 'spoiled', 'spoke', 'spoken', 'sponsored', 'spontaniety', 'sporting', 'spot', 'spots', 'spotted', 'spread', 'spring', 'sprinkle', 'sprint', 'spy', 'sq', 'square', 'squeal', 'squeeze', 'srsly', 'st', 'stabilizer', 'stacks', 'staff', 'stage', 'stand', 'standard', 'standardization', 'standing', 'star', 'starbu', 'starbucks', 'staring', 'starry', 'stars', 'start', 'started', 'starting', 'starts', 'startup', 'startupbus', 'startups', 'statement', 'states', 'station', 'stations', 'stats', 'status', 'statuses', 'stay', 'staying', 'stays', 'steady', 'stealing', 'steals', 'steampunk', 'steamy', 'stellar', 'step', 'stepped', 'stereo', 'sters', 'steve', 'stickers', 'still', 'stillman', 'stillonamacbook', 'stock', 'stogies', 'stoked', 'stole', 'stolen', 'stood', 'stop', 'stopped', 'stops', 'storage', 'store', 'stores', 'stories', 'storm', 'story', 'straight', 'strange', 'strangeproblems', 'stranger', 'strangers', 'straps', 'strategic', 'strategy', 'straw', 'stream', 'streaming', 'streams', 'street', 'streetview', 'strength', 'stress', 'stretches', 'striking', 'strip', 'strive', 'stroke', 'strong', 'structured', 'struggle', 'strums', 'stuck', 'studentsforcleanwater', 'studies', 'studios', 'study', 'studying', 'stuff', 'stumbledupon', 'stumbling', 'stunning', 'stunt', 'stupid', 'style', 'stylish', 'suasxsw', 'subscription', 'subscriptions', 'succeed', 'success', 'successful', 'succumb', 'such', 'suck', 'suckas', 'sucked', 'suckling', 'sucks', 'suddenly', 'suffered', 'suffering', 'suggest', 'suggestion', 'suggestions', 'suicidal', 'suicide', 'sullivan', 'summer', 'summit', 'sun', 'sunday', 'sundayswagger', 'sunglasses', 'sunny', 'suns', 'super', 'superbia', 'supply', 'support', 'supporting', 'suppose', 'supposed', 'supposedly', 'sure', 'surely', 'surface', 'surpassed', 'surplus', 'surprise', 'surprises', 'surrounded', 'surui', 'survey', 'survival', 'survive', 'survived', 'suspense', 'sustainability', 'sux', 'suxsw', 'svcs', 'swag', 'swarming', 'swarms', 'sweater', 'sweeeeet', 'sweeeet', 'sweepstakes', 'sweet', 'sweets', 'swift', 'swing', 'swish', 'swisher', 'switch', 'switches', 'swonderlin', 'swoon', 'swsurrogates', 'swsx', 'sxfl', 'sxflip', 'sxprotect', 'sxsh', 'sxsurrogates', 'sxsw', 'sxsw11', 'sxsw2011', 'sxsw4japan', 'sxswaccel', 'sxswbarcrawl', 'sxswbigbrands', 'sxswbuffalo', 'sxswchi', 'sxsweisner', 'sxswfail', 'sxswgo', 'sxswgood', 'sxswh', 'sxswi', 'sxswk', 'sxswmobileapps', 'sxswmoot', 'sxswmusic', 'sxswmymistake', 'sxswnui', 'sxswparty', 'sxswpass', 'sxswsa', 'sxswsex', 'sxswsmall', 'sxswtoolkit', 'sxtxstate', 'sxwsi', 'sxxpress', 'syked', 'symbian', 'symbol', 'sync', 'synced', 'synching', 'syncing', 'syncs', 'synergy', 'system', 'taariq', 'tab', 'table', 'tables', 'tablet', 'tablets', 'taccsxsw', 'tacos', 'tag', 'tagging', 'take', 'takeaway', 'taken', 'takeover', 'takes', 'takin', 'taking', 'talent', 'talented', 'talib', 'talk', 'talked', 'talking', 'talks', 'tan', 'tap', 'tapworthy', 'target', 'task', 'tastes', 'tattoo', 'tattooed', 'taught', 'taunt', 'tax', 'taxi', 'tbalinas', 'tbwasxsw', 'tc', 'tchin', 'tcs', 'tdg', 'tea', 'teach', 'teaching', 'team', 'teamandroid', 'teamandroidsxsw', 'teams', 'tear', 'teathering', 'tech', 'tech4good', 'tech_news', 'techcrunch', 'techenvy', 'techgeek', 'techie', 'techies', 'techiesunite', 'technews', 'technical', 'techno', 'technology', 'techrockstar', 'techsmith', 'tee', 'teeny', 'teeth', 'telegraph', 'teleporting', 'television', 'telework', 'tell', 'tells', 'temp', 'temperature', 'temperatures', 'temporary', 'tempt', 'temptation', 'tempted', 'tempting', 'ten', 'tenets', 'tent', 'teo', 'term', 'terminal', 'terms', 'terrible', 'test', 'tested', 'testing', 'tests', 'tether', 'tethering', 'texas', 'texasevery', 'text', 'texting', 'th', 'than', 'thank', 'thanks', 'thanksforthebrandedshades', 'thankyouecon', 'that', 'thats', 'the', 'the_daily', 'theatre', 'theem', 'thegogame', 'theindustryparty', 'their', 'theirs', 'them', 'theme', 'themed', 'themselves', 'then', 'thenextweb', 'theplatform', 'ther', 'therapy', 'there', 'thereby', 'therefore', 'these', 'thewildernessdowntown', 'they', 'thick', 'thier', 'thin', 'thing', 'things', 'thingsthatdontgotogether', 'think', 'thinking', 'thinks', 'thinmints', 'thinner', 'third', 'thirsty', 'this', 'thisisdare', 'tho', 'thomas', 'thoora', 'those', 'though', 'thought', 'thoughtful', 'thoughts', 'thousands', 'threat', 'three', 'threw', 'thrilled', 'through', 'throughout', 'throw', 'throwin', 'throwing', 'thru', 'tht', 'thumbs', 'thunder', 'thursday', 'thus', 'thwarted', 'thx', 'ticket', 'tickets', 'tidbit', 'tiff', 'tigerblood', 'tight', 'til', 'till', 'tim', 'timberlake', 'time', 'timechange', 'timed', 'timeline', 'timely', 'times', 'timing', 'tinkering', 'tiny', 'tinyurl', 'tip', 'tipped', 'tips', 'tired', 'tis', 'title', 'titles', 'tix', 'tkts', 'tm', 'tme', 'tmobile', 'tmr', 'tmrw', 'tmsxsw', 'tnw', 'tnx', 'to', 'toast', 'today', 'together', 'told', 'tomlinson', 'tomorrow', 'ton', 'tonchidot', 'tonigh', 'tonight', 'tons', 'too', 'toocoolforsxswanyway', 'toodamnlucky', 'took', 'tool', 'toolkit', 'toolongforme', 'tools', 'toooo', 'top', 'topicality', 'topics', 'topnews', 'tops', 'topspin', 'torch', 'tore', 'torture', 'torturous', 'total', 'totalitarian', 'totally', 'toting', 'touch', 'touched', 'touching', 'touchingstories', 'tough', 'tougher', 'toured', 'tournament', 'towards', 'towel', 'town', 'toy', 'track', 'tracker', 'trackpads', 'tracks', 'traction', 'tractor', 'trade', 'traded', 'tradeshow', 'traffic', 'trailer', 'train', 'trajan', 'tramplings', 'transfer', 'transient', 'transition', 'translated', 'translates', 'transparency', 'transparently', 'trashy', 'trauma', 'travel', 'travelers', 'traveling', 'traveller', 'treatment', 'trenches', 'trend', 'trending', 'trends', 'tribes', 'tried', 'tries', 'trigger', 'trip', 'tripping', 'tron', 'trophy', 'trouble', 'truck', 'trucks', 'true', 'truly', 'trumping', 'trumps', 'trust', 'trusted', 'trustworthiness', 'truth', 'try', 'trying', 'ts', 'tshirt', 'tsunami', 'tt', 'tub', 'tube', 'tuesday', 'tumblr', 'tunage', 'tune', 'tuned', 'tunehopper', 'tunes', 'turing', 'turkey', 'turn', 'turned', 'turning', 'turns', 'tuxedo', 'tv', 'tveverywhere', 'tvontheradio', 'tvs', 'tweeps', 'tweet', 'tweetcaster', 'tweetdeck', 'tweeted', 'tweethouse', 'tweetie', 'tweetignite', 'tweeting', 'tweets', 'tweetup', 'twice', 'twit', 'twitpic', 'twitter', 'twitterpower', 'twnp', 'two', 'tx', 'tye', 'type', 'typing', 'tyson', 'uber', 'uberguide', 'ubersocial', 'ubertwitter', 'ubiquitous', 'ubiquity', 'ubuntu', 'ugh', 'ughhh', 'ui', 'um', 'umassjour', 'umm', 'ummmm', 'umshini', 'un', 'una', 'unabashed', 'unadulterated', 'unbearable', 'unbelievable', 'unboxing', 'uncategorized', 'uncertainty', 'uncharged', 'unconfirmed', 'under', 'underestimate', 'underneath', 'understand', 'understanding', 'underway', 'underwire', 'undoubtedly', 'unequipped', 'unexpected', 'unfair', 'ungrateful', 'unimitated', 'unique', 'unite', 'united', 'units', 'universe', 'unleash', 'unless', 'unlisted', 'unloaded', 'unlockable', 'unlocking', 'unofficial', 'unpack', 'unpaid', 'unreal', 'unscientific', 'unsix', 'unstable', 'untapped', 'until', 'untrue', 'unveil', 'unveiled', 'unveiling', 'unveils', 'uosxsw', 'up', 'upbeat', 'upc', 'update', 'updated', 'updates', 'updating', 'upgrade', 'upgrading', 'upload', 'upon', 'uppward', 'ups', 'upset', 'upside', 'upstairs', 'ur', 'urinal', 'urs', 'urthots', 'us', 'usa', 'usability', 'usage', 'usb', 'usdes', 'use', 'used', 'useful', 'usefulness', 'useless', 'user', 'users', 'uses', 'usguys', 'using', 'ustream', 'usual', 'usurped', 'utility', 'utilize', 'utter', 'ux', 'uxdes', 'uzu', 'v1', 'v2', 'v3', 'v5', 'vacation', 'valid', 'valley', 'valuable', 'value', 'values', 'vast', 'vb', 'vcards', 've', 'vector', 'vectors', 'vegan', 'vegas', 'vending', 'venturebeat', 'venue', 'venues', 'ver', 'verizon', 'version', 'versions', 'very', 'veryslow', 'vestibule', 'veterans', 'via', 'vibe', 'vicariously', 'victims', 'vid', 'video', 'videogame', 'videogames', 'videos', 'vids', 'view', 'view512', 'viewed', 'viewing', 'views', 'vinh', 'vintage', 'vip', 'virgin', 'virginity', 'virtual', 'virtually', 'virtualoffice', 'virtualwallet', 'visigoths', 'visit', 'visiting', 'visits', 'visual', 'visualisation', 'visualization', 'visualize', 'visualizing', 'vmware', 'vodka', 'voice', 'voicefeed', 'volume', 'voluntarily', 'volunteering', 'volunteers', 'vortex', 'vote', 'voxpop', 'vp', 'vs', 'vuelta', 'vufinders', 'vuitton', 'vuvuzela', 'w00t', 'waaaaaa', 'wack', 'wait', 'waited', 'waiting', 'wakeup', 'wakeuplaughing', 'waking', 'wal', 'walk', 'walked', 'walkin', 'walking', 'walks', 'wall', 'wallace', 'walmart', 'wam', 'wandered', 'wanderer', 'wanna', 'wannabe', 'want', 'wanted', 'wanting', 'wants', 'war', 'warmer', 'warmth', 'warning', 'wars', 'wary', 'was', 'wasn', 'waste', 'wasted', 'wasting', 'watch', 'watched', 'watching', 'water', 'waterproof', 'watson', 'wave', 'waves', 'way', 'ways', 'waze', 'we', 'wean', 'wearing', 'weasel', 'weather', 'web', 'web3', 'web30', 'webber', 'webdoc', 'webkit', 'webmail', 'webmaster', 'webmasters', 'website', 'websites', 'webvisions', 'wed', 'week', 'weekend', 'weeks', 'weeping', 'weight', 'weinschenk', 'weird', 'welcome', 'welivehere', 'well', 'went', 'were', 'weren', 'wesley83', 'west', 'weve', 'wew', 'whale', 'what', 'whatcha', 'when', 'where', 'whether', 'which', 'while', 'whimsical', 'white', 'whiteboarding', 'who', 'whoa', 'whole', 'wholistic', 'whoohoo', 'whoooooo', 'whoops', 'whowillrise', 'whrrl', 'why', 'wi', 'wider', 'widfy', 'widgets', 'wife', 'wifi', 'wii', 'wil', 'wild', 'wilderness', 'will', 'williams', 'willing', 'willpay', 'willpower', 'wilting', 'win', 'winamp', 'windows', 'windows7', 'wine', 'wings', 'winner', 'winners', 'winning', 'wins', 'wintel', 'winwin', 'wipes', 'wired', 'wireless', 'wires', 'wisconsin', 'wise', 'wish', 'wishful', 'wishing', 'with', 'withdrawal', 'within', 'withme', 'without', 'witnessed', 'witty', 'wjchat', 'wk', 'wkd', 'wkend', 'wknd', 'woah', 'wodpress', 'woes', 'wohooo', 'woke', 'wolfenstein', 'wolfram', 'woman', 'women', 'won', 'wonder', 'wonderful', 'wondering', 'wonders', 'woo', 'woohoo', 'wooooo', 'woops', 'woot', 'word', 'wordnerd', 'wordpress', 'words', 'work', 'worked', 'workers', 'workin', 'working', 'works', 'workspace', 'world', 'worlds', 'worldwide', 'worn', 'worried', 'worry', 'worse', 'worst', 'worth', 'worthwhile', 'wot', 'would', 'wouldn', 'wow', 'wowwwwww', 'wozniak', 'wp7', 'wr', 'wrap', 'wrapper', 'wrapping', 'write', 'writer', 'writes', 'writing', 'wrong', 'wrote', 'wsj', 'wssxsw', 'wtf', 'wundertablet', 'wut', 'wwsxsw', 'www', 'x6t1pi6av7', 'xbox', 'xd', 'xipad', 'xm', 'xmas', 'xml', 'xoom', 'xperia', 'xplat', 'xwave', 'ya', 'yai', 'yall', 'yawn', 'yay', 'yea', 'yeaayyy', 'yeah', 'year', 'years', 'yeasayer', 'yeay', 'yellow', 'yelp', 'yelping', 'yep', 'yes', 'yesterday', 'yet', 'yield', 'yikes', 'yo', 'yobongo', 'yonkers', 'york', 'you', 'youneedthis', 'your', 'yourself', 'youtube', 'yowza', 'yr', 'yrs', 'yummy', 'yup', 'zaarly', 'zaarlyiscoming', 'zagg', 'zaggle', 'zap', 'zappos', 'zazzlesxsw', 'ze', 'zelda', 'zeldman', 'zero', 'zimride', 'zing', 'zip', 'zite', 'zms', 'zombies', 'zomg', 'zone', 'zoom', 'zzzs']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dwtgjTBeH0Ny"
   },
   "source": [
    "#### Tip: To see all available functions for an Object use dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ShA6D8jKH0N5"
   },
   "source": [
    "### 18. Find out how many Positive and Negative emotions are there.\n",
    "\n",
    "Hint: Use value_counts on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q7LAl5pzH0N6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positive emotion    2978\n",
       "Negative emotion     570\n",
       "Name: is_there_an_emotion_directed_at_a_brand_or_product, dtype: int64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.is_there_an_emotion_directed_at_a_brand_or_product.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUvgj0FoH0N9"
   },
   "source": [
    "### 19. Change the labels for Positive and Negative emotions as 1 and 0 respectively and store in a different column in the same dataframe named 'Label'\n",
    "\n",
    "Hint: use map on that column and give labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YftKwFv7H0N9"
   },
   "outputs": [],
   "source": [
    "data['Label'] = data.is_there_an_emotion_directed_at_a_brand_or_product.map({'Positive emotion':1,'Negative emotion':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3YErwYLCH0N_"
   },
   "source": [
    "### 20. Define the feature set (independent variable or X) to be `text` column and `labels` as target (or dependent variable)  and divide into train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lNkwrGgEH0OA"
   },
   "outputs": [],
   "source": [
    "X = data['text']\n",
    "Y = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q5nlCuaaH0OD"
   },
   "source": [
    "## 21. **Predicting the sentiment:**\n",
    "\n",
    "\n",
    "### Use Naive Bayes and Logistic Regression and their accuracy scores for predicting the sentiment of the given text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2AbVYssaH0OE"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2661, 5046)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer()\n",
    "x_train_dtm = vect.fit_transform(x_train)\n",
    "x_test_dtm = vect.transform(x_test)\n",
    "x_train_dtm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ktXrLhmOH0Of"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8647125140924464\n",
      "[[ 37 109]\n",
      " [ 11 730]]\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "nb.fit(x_train_dtm, y_train)\n",
    "y_pred_class = nb.predict(x_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "clv2X0kKH0Ok"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8523111612175873\n",
      "[[ 45 101]\n",
      " [ 30 711]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ssbru\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(x_train_dtm, y_train)\n",
    "y_pred_class = lr.predict(x_test_dtm)\n",
    "\n",
    "# calculate accuracy\n",
    "print(metrics.accuracy_score(y_test, y_pred_class))\n",
    "print(metrics.confusion_matrix(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sw-0B33tH0Ox"
   },
   "source": [
    "## 22. Create a function called `tokenize_predict` which can take count vectorizer object as input and prints the accuracy for x (text) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "okCTOs1TH0Oy"
   },
   "outputs": [],
   "source": [
    "def tokenize_test(vect):\n",
    "    x_train_dtm = vect.fit_transform(x_train)\n",
    "    print('Features: ', x_train_dtm.shape[1])\n",
    "    x_test_dtm = vect.transform(x_test)\n",
    "    nb = MultinomialNB()\n",
    "    nb.fit(x_train_dtm, y_train)\n",
    "    y_pred_class = nb.predict(x_test_dtm)\n",
    "    print('Accuracy: ', metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JxZ8jfPEH0O0"
   },
   "source": [
    "### Create a count vectorizer function which includes n_grams = 1,2  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kdCyAN_IH0O0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  25736\n",
      "Accuracy:  0.874859075535513\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(ngram_range=(1, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "axepytmgH0O4"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english'  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HToGkq7vH0O4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  4806\n",
      "Accuracy:  0.8624577226606539\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(stop_words='english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOIlJRxoH0O7"
   },
   "source": [
    "### Create a count vectorizer function with stopwords = 'english' and max_features =300  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fUhff-oH0O8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  300\n",
      "Accuracy:  0.8094701240135288\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(stop_words='english',max_features=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S2KZNWVkH0PA"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and max_features = 15000  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3v9XD082H0PB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  300\n",
      "Accuracy:  0.7643742953776775\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(ngram_range=(1, 2),max_features=300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "We3JK_SRH0PO"
   },
   "source": [
    "### Create a count vectorizer function with n_grams = 1,2  and include terms that appear at least 2 times (min_df = 2)  and pass it to tokenize_predict function to print the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fUHrfDCyH0PP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:  8297\n",
      "Accuracy:  0.8714768883878241\n"
     ]
    }
   ],
   "source": [
    "tokenize_test(CountVectorizer(ngram_range=(1, 2),min_df=2))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "R8_Internal_Lab_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
